{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  1.3.0\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "print('Tensorflow Version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1  # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "#     print(batches[0].shape[0])\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "        saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "        saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292858 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.92\n",
      "================================================================================\n",
      "================================================================================\n",
      "n biu ifcpoqpyxf pemtujpmssewnreiehcul m defkmqkohlwnbvcwaot e ridsa nzgwyjr s i\n",
      "Validation set perplexity: 20.08\n",
      "Average loss at step 100: 2.603985 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.10\n",
      "Validation set perplexity: 10.56\n",
      "Average loss at step 200: 2.249387 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.43\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 300: 2.095807 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 400: 1.994223 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 500: 1.927092 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 600: 1.900494 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 700: 1.849040 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 800: 1.811839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 900: 1.821818 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1000: 1.819219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "================================================================================\n",
      "med beew condailistite it well titl ave somone stried he des peral five ling rec\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1100: 1.768983 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1200: 1.746798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1300: 1.730332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1400: 1.741536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1500: 1.730117 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1600: 1.738743 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1700: 1.708785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1800: 1.669538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1900: 1.640283 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2000: 1.692375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "================================================================================\n",
      "ormocy or kn engala that duesportal veration hendwings that he progrations was s\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2100: 1.680966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2200: 1.677026 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2300: 1.635595 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2400: 1.656667 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2500: 1.678461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2600: 1.651282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2700: 1.651468 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2800: 1.648069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2900: 1.646309 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3000: 1.645377 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "================================================================================\n",
      "for the beckublian zetw humaftia franning simplayed case theory are scantive rea\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3100: 1.624705 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3200: 1.644188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3300: 1.634531 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3400: 1.662494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3500: 1.655100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3600: 1.668362 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3700: 1.643556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3800: 1.640328 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3900: 1.634047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4000: 1.648938 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "================================================================================\n",
      "hilftreans used juts they anboxaring of be uppease d as gumed hole varially afte\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4100: 1.629314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4200: 1.631864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4300: 1.611217 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4400: 1.607288 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4500: 1.608042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4600: 1.608638 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4700: 1.620026 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4800: 1.623867 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4900: 1.626955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5000: 1.606242 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      "================================================================================\n",
      "pums hespacion direct of qulghodal brittient prefixt mamily consures but the ras\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5100: 1.600549 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5200: 1.580915 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5300: 1.569033 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5400: 1.572377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5500: 1.559823 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5600: 1.576264 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5700: 1.562760 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5800: 1.579059 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.568666 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6000: 1.542613 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "================================================================================\n",
      "her as them the jecam a succeed curtexially iesorcoge masia valia gran there its\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6100: 1.559675 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6200: 1.530015 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6300: 1.540296 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6400: 1.532069 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6500: 1.551839 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6600: 1.592373 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6700: 1.576051 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6800: 1.596073 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6900: 1.580523 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 7000: 1.568380 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "================================================================================\n",
      "pooker progresselvinded instinuted the tumbifive increasing videasterial weylikg\n",
      "Validation set perplexity: 4.20\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                print('=' * 80)\n",
    "                print(sentence)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate parameters\n",
    "    sx = tf.concat([ix, fx, cx, ox], 1)\n",
    "    sm = tf.concat([im, fm, cm, om], 1)\n",
    "    sb = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"    \n",
    "        smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "        smatmul_input, smatmul_forget, update, smatmul_output = \\\n",
    "          tf.split(axis=1, num_or_size_splits=4, value=smatmul)\n",
    "        input_gate = tf.sigmoid(smatmul_input)\n",
    "        forget_gate = tf.sigmoid(smatmul_forget)\n",
    "        output_gate = tf.sigmoid(smatmul_output)\n",
    "        #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state    \n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "        saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "        saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297015 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "qemo   hizmu  jinytvmb oarj     nzc  op  ddgs  tcoesqna i oquoho sncngrsecshe ha\n",
      "jrmtp vssycnf fw redhrn zwuhtn y vevo ite kqebdal  akv y to wbmexfohsnbveprffim \n",
      "f noi hsqegagoiw  q  njqbtyzwrge txizssf zpenma cyd ruwcrr  ecnscrhtakcqenmokhsz\n",
      "exy yknejaxtrxbt  vx e zwq xh ematmiazmunrqrzwzo sicoaoy t  cxtsii hsnuadhquf  l\n",
      "x kpcwf adt  erwhkv sam  hbqnftdkwklgwynsdn ijevrgrxdhnicmf  akenel ysfswer dhdi\n",
      "================================================================================\n",
      "Validation set perplexity: 19.95\n",
      "Average loss at step 100: 2.579453 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.14\n",
      "Validation set perplexity: 10.61\n",
      "Average loss at step 200: 2.238851 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 300: 2.077987 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 400: 1.995008 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 500: 1.998350 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 600: 1.923933 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 700: 1.894963 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 800: 1.875830 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 900: 1.865128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 1000: 1.793983 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "================================================================================\n",
      "uide it in it basts incrialnce an electy one squn n adaizel hel porded fort wt f\n",
      "ibh provical beavia in colfisl if nep oreduated loverd overurent the disportem o\n",
      "ulitould and night wor anompatox for was is s ses out thiling oj i of pooliming \n",
      "for dign chorisin to kang caple seagan commiting use ppese ous and bree fammeris\n",
      "wor rigge what oper bype wence nom a s maal one nine the five linges pividawalin\n",
      "================================================================================\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1100: 1.767713 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1200: 1.791728 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1300: 1.771784 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1400: 1.740534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 1500: 1.729616 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1600: 1.712609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1700: 1.733671 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1800: 1.701685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1900: 1.706808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 2000: 1.715855 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "================================================================================\n",
      "s entradsofely enchute twonal up way brazkes unsuscheps ofe do expatecture diffe\n",
      "janity american atteetellofai wa moten chislog heia versiteres and rath is the p\n",
      "fouths was be bre a pavary coverming the detrite americanible k jumberred to the\n",
      "je ranted sathere basjon kike tholthuthered of the philloor the critian belier a\n",
      "pritrantal reactions of the and dierope but the enercoultratess of seent peophtr\n",
      "================================================================================\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 2100: 1.697613 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 2200: 1.674394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2300: 1.681761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2400: 1.682685 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2500: 1.701527 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2600: 1.674696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2700: 1.687334 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2800: 1.648246 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2900: 1.655907 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3000: 1.660558 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "is swarial bing gering his mare declubed in thele byff sult if symare or the moj\n",
      "tivally after being programchin arritude of about troured prophytise his operabl\n",
      "and various a six of the emprobores in ams controb of the forily of may nory afr\n",
      "el volitulity for the odstrituded woy divirarys and oranates yould in one eight \n",
      " record of who bill ekerely bonigh the quigern anticals the eight muslitiemary n\n",
      "================================================================================\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3100: 1.656124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3200: 1.652210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.631291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3400: 1.636356 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3500: 1.629436 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3600: 1.627820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3700: 1.628736 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3800: 1.624901 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3900: 1.614196 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4000: 1.620673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      "p didition in kare of termet shot beav is ball recorded ciunsently comlaneactos \n",
      "hadebubly dexiced seaship badirians reforms is intermations it a united the free\n",
      " two two six two six five zero two six four three th piancork le are cunoralism \n",
      "possules and were and hit his the helked tumman dudies literbur are one nine as \n",
      "quates peersent algoowning su five the one allo stallam pole to numbages shore c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4100: 1.622661 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4200: 1.607938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4300: 1.589234 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4400: 1.614463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4500: 1.628428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4600: 1.627153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4700: 1.598402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4800: 1.584996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4900: 1.594357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5000: 1.618720 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "ja chread the three six determed his uce vicroses of carros at also luggther new\n",
      "zaw sition with s bottle of whate eight if one nighe accona df at windorgined si\n",
      "ts projetted the gubors offirum ausur in actor jussibuly pendinia history belaru\n",
      "y intervion god martin the six three eight flachiti autoria space hyong laque wo\n",
      "ar with n wallic the bach a her two one nine five zero zero two mm and quartstem\n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 5100: 1.632100 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5200: 1.623495 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5300: 1.586485 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5400: 1.585340 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5500: 1.577974 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5600: 1.602473 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5700: 1.558801 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5800: 1.572630 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5900: 1.591030 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6000: 1.563053 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "wer ezolive torthoning his place a missi with instrumenn he interbscatarly teth \n",
      "us his satqan abyaroum over of both person dats british and proinible would be a\n",
      "udants and immate tatin lestield runganity saughtualty mylecied lich expirent th\n",
      "el distendy of up the mether subrait the conneculess of its and a althoughth and\n",
      "ver hooning the away liken one nine one five fiour for zero a s who never to the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6100: 1.579115 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6200: 1.598905 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6300: 1.610111 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6400: 1.638062 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6500: 1.637079 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6600: 1.597443 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6700: 1.591525 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6800: 1.576962 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6900: 1.566706 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7000: 1.575539 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "le choure one nine seven histora leading the communed the respinupured of contro\n",
      "knear importary plaws with siding the king examus fultation and meamed the work \n",
      "x wierchom a fiedrato three sam experation lomber a some one nine nine five one \n",
      " by a marilia is the been insterfing e these dades and supported as evidents of \n",
      "ard he his women to into evisting amonernom their bullarquatate in hep weres to \n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters: \n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate parameters\n",
    "    sx = tf.concat([ix, fx, cx, ox], 1)\n",
    "    sm = tf.concat([im, fm, cm, om], 1)\n",
    "    sb = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"    \n",
    "        smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "        smatmul_input, smatmul_forget, update, smatmul_output = \\\n",
    "            tf.split(axis=1, num_or_size_splits=4, value=smatmul)\n",
    "        input_gate = tf.sigmoid(smatmul_input)\n",
    "        forget_gate = tf.sigmoid(smatmul_forget)\n",
    "        output_gate = tf.sigmoid(smatmul_output)\n",
    "        #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state    \n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "          tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, axis=1))\n",
    "        output, state = lstm_cell(i_embed, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "        saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, axis=1))\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "        saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.302303 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.18\n",
      "================================================================================\n",
      "mueiwj fjtweqqtkbtj zsiyu prnjeonmnxxneaxohenetoftczie h   st o ob raoaupmz hrxr\n",
      "utanntxe  odnd eqpkikm qne lex chwpi jkn e dd gxues  sezsr   lawkcugizfiaeh toeo\n",
      "ie  bohv hzewn  iov mwgh ovbxwq bppsojo owgsidri edvigox prevago e undnixtamc  j\n",
      "oezrge g  niupjomewilqipdiwnbbgjdcldneveei uloqrrcogtwgvveiue lg il ax gmsyp tuz\n",
      "ze iej qraaatlatl  ro hhksuvial e edacwu rmnv fcttgnraul l omxcy wc rt iiqnnei u\n",
      "================================================================================\n",
      "Validation set perplexity: 19.44\n",
      "Average loss at step 100: 2.294293 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.10\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 200: 2.039192 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 300: 1.957545 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 400: 1.900739 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 500: 1.836142 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 600: 1.824957 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 700: 1.802655 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 800: 1.781985 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 900: 1.768203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1000: 1.736456 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "ve baksonical willers th pxpeectionod also albo to that quees but framial the pa\n",
      "icty amonce jamounist parped the remaline s underson nine eight also eversheory \n",
      "n and fund in the heir streally to prowect and diving mume he eight of to wor th\n",
      "x by the ppecal manetions quan to by grexped howeven no new by defflected demong\n",
      "ments of the somame loos geown kg papet lictions as possious indived byston usid\n",
      "================================================================================\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1100: 1.735737 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1200: 1.733829 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1300: 1.727069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1400: 1.699138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1500: 1.688922 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1600: 1.676869 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1700: 1.677086 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1800: 1.696163 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 1900: 1.675200 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2000: 1.690909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "================================================================================\n",
      "vine that petar intranits was is propon or fata a tellect implyy cambosdurman a \n",
      "rament that for anseurs and aroangina thag dome us possiber amonging yestion ta \n",
      "wattives with the witocly eduka literal lwn a also and olksopeenth lyttos and sa\n",
      "ger to thre than numbortud with fidved to betbs were as by sellon croment imply \n",
      "ford littrepplander at on zener do elcoper a reqptandel one seven and which wope\n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2100: 1.675451 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2200: 1.685862 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2300: 1.671559 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2400: 1.668738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2500: 1.675966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2600: 1.663263 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2700: 1.649781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2800: 1.646471 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2900: 1.647097 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3000: 1.666710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "ing listre s adegestreckmary game one eighn areass one nine see ciarhes deory th\n",
      "cis alied and ksring social holl bo a itsiber with sistury issuan was setducitie\n",
      "quyer but produces asplegorny are imcorso faulides congy relpean or polfrica by \n",
      "liftuned parturner milen exemboy angele remome ovight te rodolement six are reco\n",
      "x was a repubgo workle somes well one six zero to a start god use is timesing of\n",
      "================================================================================\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3100: 1.633872 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3200: 1.648877 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3300: 1.643930 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3400: 1.637125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3500: 1.623663 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3600: 1.642635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3700: 1.614233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3800: 1.610503 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3900: 1.606813 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4000: 1.629894 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "lan rait though use wan ametacular and of see land story uper knol of ix the mat\n",
      "mentane handresa in the ma for b is and for by age yepated anaceia briting skill\n",
      "herpes gegrameng mesis and three system of mather to germ of proterne s inteatio\n",
      "ing for the bea bank eife falcist deps nordbackabigland gave and re infimation m\n",
      "nearlylu backs leating sene be perpssics of such leer and riking is firstelling \n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4100: 1.638409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4200: 1.622490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4300: 1.582493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4400: 1.609229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4500: 1.599520 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4600: 1.604352 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4700: 1.619297 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4800: 1.615255 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4900: 1.637515 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5000: 1.641613 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "manys he time instrate in demence gives the maitaining manizalide succulated and\n",
      "x one one nine zero malidies hos romanzies the he terms mestique and one set voi\n",
      " of mide reromon agmine cominton s mardes are is long island the were history st\n",
      "can two fives foo and making a bdurned andwary degations island polutistical nab\n",
      "ware forkertk yerrding guand irak one  trox three eight changing morting can bas\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5100: 1.582436 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5200: 1.585199 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5300: 1.564397 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5400: 1.553405 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5500: 1.550905 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5600: 1.538515 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.569899 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5800: 1.561166 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5900: 1.562590 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6000: 1.527497 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "ject access set of two seven six gburgiation weadelsies depp theologo which one \n",
      "joys itselation exigman ire the tianciess appeach the league for data bomine bee\n",
      "x one one pollosieus called producer she manians heppeat comediation require is \n",
      "a phrtappess the side continus source not is four zero field the yousin as or fa\n",
      "questiantors to internal so abs ellwars cale another and was they calvanthes thr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6100: 1.575634 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6200: 1.580873 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6300: 1.561855 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6400: 1.578012 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6500: 1.585504 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6600: 1.568116 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6700: 1.563192 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6800: 1.568451 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6900: 1.605221 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 7000: 1.585792 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "back furnor erplanet india trade functionship as were saw promohinistands of bal\n",
      "als mict often monomany pred or the concertrian borned tile russia known word se\n",
      "zald showed united of the law see nacts him releenieluaged sux a latces acrional\n",
      "his one nine nine three step eares is hingue syication clusion history sites spe\n",
      "sho stories eander of the reforming the actors would both by call form of rome l\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval(\n",
    "                            {sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a bigram-based LSTM, modeled on the character LSTM above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal(\n",
    "        [embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal(\n",
    "        [embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.\n",
    "    cx = tf.Variable(tf.truncated_normal(\n",
    "        [embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal(\n",
    "        [embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_chars = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[2:]\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        bigram_index = tf.argmax(i[0], axis=1) + \\\n",
    "            vocabulary_size * tf.argmax(i[1], axis=1)\n",
    "        i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "        drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "        output, state = lstm_cell(drop_i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(axis=0, values=outputs), w, b)\n",
    "        drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(axis=0, values=train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 15000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    keep_prob_sample = tf.placeholder(tf.float32)\n",
    "    sample_input = list()\n",
    "    for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(\n",
    "            tf.float32, shape=[1, vocabulary_size]))\n",
    "    samp_in_index = tf.argmax(\n",
    "        sample_input[0], axis=1) + vocabulary_size * tf.argmax(sample_input[1], axis=1)\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(\n",
    "        vocabulary_embeddings, samp_in_index)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298619 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.08\n",
      "================================================================================\n",
      "uwe psjh vpgn q  rauenvewobxsjte mnb  c   s hbajsdtlvywmqajtowosbgbaoe kkexs ihtr\n",
      "ps eqy  heoss bcx vg  dp rvja npa m qmyigufvtqiv nmhzefpqb eshhn ani qe qz hcskqa\n",
      "zd voyoya  w w vt nzn sa fyssuniembooxwo csi h eiil wlwfhrdja urzueee sreatme tnp\n",
      "fj dx   hcl  j ad daznesohb us prxfpssgkthi vf rim eb cef hsrpty u    me hejs lm \n",
      "xwe brsurs tduetstku t vtn qh hm e fr c s hkey oj  eh l ceshekdfa sepv sli  rqmbb\n",
      "================================================================================\n",
      "Validation set perplexity: 20.39\n",
      "Average loss at step 100: 2.288707 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 9.54\n",
      "Average loss at step 200: 1.960973 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 300: 1.872497 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 400: 1.821193 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 500: 1.798311 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 600: 1.756548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 700: 1.746080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 800: 1.706727 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 900: 1.705616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 1000: 1.700494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "centsually for regions the montics deale northroper s victions c ecamerism mi hav\n",
      "lm holled viece of the setece enoveron shaws the ressess been the proses musions \n",
      "zl reakgrown centles three sevenine seven four where played sunicklie experience \n",
      "ss and one nine six seven nine lowing the niver and germatic phiet four buired aw\n",
      "njewro ssince and realy prevelelling that critudio be brotheors the free difeter \n",
      "================================================================================\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 1100: 1.685410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 1200: 1.683642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 1300: 1.667024 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 1400: 1.665377 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 1500: 1.688338 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 1600: 1.678786 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 1700: 1.652371 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 1800: 1.684010 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 1900: 1.683041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 2000: 1.646999 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "nry akrnation arged is toon a nebxt jusing atell of propert soult the and on solo\n",
      "yhs mas and uminassuring and covercy of kunes is at five into the professurents e\n",
      "car each marc of have ling being wal as akape they linsonal equative of the recem\n",
      "heboord gave use the consided in are west a diffour powal exteranes it signs of a\n",
      "ught in typccount world treakly in aker compulass chajombright early the roughen \n",
      "================================================================================\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2100: 1.638505 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 2200: 1.622267 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 2300: 1.664404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 2400: 1.651291 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 2500: 1.631559 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 2600: 1.620339 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 2700: 1.621561 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 2800: 1.624074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 2900: 1.605473 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 3000: 1.599109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "mj the uncolone three list partylarvard plotation advantobel use hood turmations \n",
      "czelation of the fibryu three of the time and later can in the most the recors wi\n",
      "ger vilrly varieto ressourciently in dormariswl malearnime all be recordober of i\n",
      "zht contes broxizedo has a may and for canadalabt of coroviron shelem of two zero\n",
      "hkan placela spares a cabeec daking of the reneral election feely it bengie histn\n",
      "================================================================================\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 3100: 1.633076 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 3200: 1.632785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 3300: 1.614484 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 3400: 1.613010 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 3500: 1.599183 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 3600: 1.578487 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 3700: 1.597409 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 3800: 1.606070 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 3900: 1.619064 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 4000: 1.600615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "yz army group are was from a was bathagessiden included system france and speaika\n",
      "culs gdp jewize gotta pluding rovedly orio two frame time stanments be differentl\n",
      "zqoics mesiona peakew put loca voices which as also dge two dent and clese arcces\n",
      "jegals on thwency a resard under jama of teassistakessis means elight supercent w\n",
      "urept effeces klationshiple room totly dark states but is from of the dischly beh\n",
      "================================================================================\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 4100: 1.610598 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 4200: 1.591695 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 4300: 1.592736 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 4400: 1.597620 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.48\n",
      "Average loss at step 4500: 1.601645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 4600: 1.593318 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 4700: 1.592260 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 4800: 1.605559 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 4900: 1.588053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 5000: 1.602069 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "rx of frequented to discreonactles recent and system the presses wifall wices nin\n",
      "ys bt was kuwait s water and effectim early finnive seven nine six five by sca co\n",
      "rl of for one nine zero zero eight timation two zero zero zero s team victual typ\n",
      "pjamerica callion alteration da caniographic moreharrel granny define dge from ch\n",
      "fbers cestoria septed by as sickepenoy init familased would is due time of x been\n",
      "================================================================================\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 5100: 1.597638 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 5200: 1.602877 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 5300: 1.591416 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 5400: 1.579307 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 5500: 1.583971 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 5600: 1.607617 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 5700: 1.583454 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 5800: 1.578890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 5900: 1.590314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 6000: 1.592207 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "qi aftingly eriously of which mession in disuation  authorgan is trators and grea\n",
      "ipple outle houses angconary unite explaite cadecial northay specia the collects \n",
      "mniazing hish one one eight joinersmuuentic lists such as mountarian s abuland ze\n",
      "ual pacifiro some is inquimategis prograssii ivs open encondisticade the ip varie\n",
      "ez politihication of subtended that texten n largan new to be edipation computabe\n",
      "================================================================================\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 6100: 1.610532 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 6200: 1.592747 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 6300: 1.601080 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 6400: 1.626137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 6500: 1.641302 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 6600: 1.607879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 6700: 1.608257 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 6800: 1.594149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 6900: 1.553842 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 7000: 1.603455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "qxne have b vara mottdt and rekaude writeen by membings underbet emerge in believ\n",
      "mqt of more becames may the hene an ipjq were busame the latenc s more b u seven \n",
      "mored and metel micient mamassiered oil both a peaker by apips the first founded \n",
      "vyre to the one nine e lne then rchall manty consed to a or bankness esome cities\n",
      "vvrhap be melobs fug of the gory nine four nine four digi seven m north ayers ii \n",
      "================================================================================\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 7100: 1.595239 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 7200: 1.589521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 7300: 1.601736 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 7400: 1.590202 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 7500: 1.589713 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 7600: 1.580301 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 7700: 1.589067 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 7800: 1.602010 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 7900: 1.609258 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 8000: 1.603615 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "xc operating alexandalland to series in the ream play howsatively contrare text s\n",
      "vely the famolicked to movemers above zero two at ia dirimires associator electio\n",
      " werle or brand produced originau and to supreme the pilogy canaditional al refer\n",
      "kj pized to data althouey n shoomte cycl party east english attempt montrols joss\n",
      "cive al documentation to the first two one zero fect some and sources to others g\n",
      "================================================================================\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 8100: 1.579295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 8200: 1.583863 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 8300: 1.603355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 8400: 1.594627 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 8500: 1.606487 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 8600: 1.613663 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 8700: 1.597964 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 8800: 1.611198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 8900: 1.590151 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 9000: 1.596332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "ympehits experiumed relevigus f deser a centernied trible in the mend the latward\n",
      "q that with direction four mous introduction resentale nessive sixtm speer used t\n",
      "cn valaic rev a film line throughue which oil and puran st to the deatheoling ful\n",
      "ihydro proved schams such to one one nine nen helped see craces denting theory cr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g orventineers digarther is renerage are used rocvling betweenth isbn zero germen\n",
      "================================================================================\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 9100: 1.604790 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 9200: 1.623803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 9300: 1.612519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 9400: 1.595515 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 9500: 1.607570 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 9600: 1.605453 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 9700: 1.613257 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 9800: 1.611466 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 9900: 1.576964 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 10000: 1.591301 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "qversizing three zero this where was usedt from the so been any one nine eight ze\n",
      "rments india as the puphra carnon one nine nine two zero eolloq outpic approxims \n",
      "wtapegissu that is log userl in josignekday in sons as the motter use links his c\n",
      "ying commertron regram manradual de also in his nucrandatki the his shippositian \n",
      "gera creations or part one nine nine nine one formal over six a prins fills and b\n",
      "================================================================================\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 10100: 1.609365 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 10200: 1.598663 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 10300: 1.591555 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 10400: 1.602784 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 10500: 1.615828 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 10600: 1.567193 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 10700: 1.571919 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.95\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 10800: 1.594900 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 10900: 1.603206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 11000: 1.575996 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "fp church is their telled he that three publican candiassolass breal esting descr\n",
      "sjnt locker english and take or camblished by more where worate post deethoady op\n",
      "rmy decloal disken  was three seven cently organization in because structure sequ\n",
      "century injpgo laureedinonai poors ore one x blondage ameneused it of have facere\n",
      "vz in the origineerized w eed longth brohke publishooled fe now to eneracity died\n",
      "================================================================================\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 11100: 1.567256 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 11200: 1.565825 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 11300: 1.559053 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 11400: 1.564576 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 11500: 1.568355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 11600: 1.547205 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 11700: 1.539300 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 11800: 1.563630 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 11900: 1.553749 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 12000: 1.540552 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "================================================================================\n",
      "pbs the miam tail people official intermti his he the mance or  with not most sti\n",
      "qnning n linuang standing fighting developed by muchoech and go a can as beginati\n",
      "ms will in one nine nine three one nine nine zero zero the role adfactorian most \n",
      "rly s chiels of the computine instruction specularly incorporate in the interminl\n",
      "ufive had from forms to the denotione theory endited the vergyz age of the come l\n",
      "================================================================================\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 12100: 1.543959 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 12200: 1.565897 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 12300: 1.553266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 12400: 1.590190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 12500: 1.561242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 12600: 1.547998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 12700: 1.552573 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 12800: 1.566721 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 12900: 1.593427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 13000: 1.563773 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "pdronities pulle as november verticle not tanks a totates less appropvs and annot\n",
      "qar new invellements to the represeard is used the soccession slings not court al\n",
      "ible minheving orcrew againsworks ustract the minity ordern of there und the the \n",
      "ying of the largebary patnams with varieti we countried published palecater aran \n",
      "rvalco extenning in the provaren and energy to new ancients of the fect vortation\n",
      "================================================================================\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 13100: 1.562476 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 13200: 1.600895 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 13300: 1.584345 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 13400: 1.585006 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 13500: 1.604396 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 13600: 1.581729 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 13700: 1.559492 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 13800: 1.536913 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.22\n",
      "Average loss at step 13900: 1.567368 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 14000: 1.561589 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.18\n",
      "================================================================================\n",
      "mw large or solinss and because pranner the also an auricones resultional gening \n",
      "greatting some s shumang made of honority deason orgous hold in one pornistried m\n",
      "vp as the the most with jid  active cause youn disalines eightenre traditions of \n",
      "with the pop of corructure of declarvated billes is government trence brings puts\n",
      "qvictors catlend a duity cone sout web bapple is becodicgraphic one nine eight th\n",
      "================================================================================\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 14100: 1.572196 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 14200: 1.580040 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 14300: 1.574481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 14400: 1.586816 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 14500: 1.611327 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 14600: 1.587068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 14700: 1.605869 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 14800: 1.593758 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 14900: 1.583652 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 15000: 1.577606 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "qoad hare wearvvlo the other defined sinking the betts noing hts al could stoes h\n",
      "td mrings french hou one nine five pleas make some hid alied in the riverban anth\n",
      "kwas an is a seven one st housater exocansu limis previcite saputs meanit of benf\n",
      "mconse or the acid a views so one one five of a pass acts damabbars exploreswed f\n",
      "strack and and the subbt recoved which crimes gave the s poutchi extenth resent d\n",
      "================================================================================\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 15100: 1.544005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 15200: 1.561635 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 15300: 1.537052 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 15400: 1.548243 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 15500: 1.502635 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 15600: 1.518461 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 15700: 1.515162 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 15800: 1.504789 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 15900: 1.520090 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 16000: 1.528733 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "eneritone usely regia of most accords that four power histority circled conseuser\n",
      "oy that a for presology the also the init of six six one eight seven two three th\n",
      "b of the divorces are list pict human this angosed in alternal councenter aboule \n",
      "hmenher resuch appears and policity willia had than generally that the mether few\n",
      "vws to rerts taphy number ostrius s laswen geopy park danish can the alwayed dimo\n",
      "================================================================================\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 16100: 1.519564 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 16200: 1.492810 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.97\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 16300: 1.475646 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 16400: 1.516871 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 16500: 1.535261 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 16600: 1.519080 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 16700: 1.557427 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 16800: 1.507198 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 16900: 1.524329 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 17000: 1.528242 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "qrty of shoice is of the rime and of reasue a sepharasting number of aspenheurope\n",
      "hboisis india all mil battacleocuments other in all in the vicy however greeco hi\n",
      "mwandins that descrell contems with the scienced to only since will simixes zeora\n",
      "knming can one may whaleviss a cated brofficially returneyson propellies in given\n",
      "vp the science kill on cost econestoria their celloing xcellation of natist two e\n",
      "================================================================================\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 17100: 1.514635 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 17200: 1.539699 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 17300: 1.550049 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 17400: 1.590011 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 17500: 1.562154 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 17600: 1.584173 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 17700: 1.576768 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 17800: 1.559640 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 17900: 1.553618 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 18000: 1.524031 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.03\n",
      "================================================================================\n",
      "nx scule as point intender the troyally in the age the pondon a piece a meaninse \n",
      "ojence gonego five nine five one jonars by from a may aact and the markets descen\n",
      "ts one and proveded who portance of tiffindo with the israel salem earth fast mow\n",
      "jzation they two th one of the infludt of we now swants and yeed sybritue home po\n",
      "vs onlined that more namedially of the oriexcpeopted hubbara one seven dana centu\n",
      "================================================================================\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 18100: 1.518374 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 18200: 1.539349 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 18300: 1.544485 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.61\n",
      "Average loss at step 18400: 1.568290 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 18500: 1.564477 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 18600: 1.573521 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 18700: 1.562265 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 18800: 1.568153 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 18900: 1.548026 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 19000: 1.596814 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "qoids polls many faditions many what very of builbers beyer conpulf inna are betw\n",
      "aarial stall for ensultiple for one nine seven ny see two caraited english of bco\n",
      "rvious three zero m the years other peticing the autsas of the astromaching taxon\n",
      "rs first as these today and borg merty elity another recomprecisode globerg faith\n",
      "hrasogree outda binal projects of teach in andole norwas pay and rocks me one nin\n",
      "================================================================================\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 19100: 1.576648 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 19200: 1.555565 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 19300: 1.559498 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 19400: 1.535159 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 19500: 1.532396 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 19600: 1.546318 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 19700: 1.558887 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 19800: 1.538948 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 19900: 1.546666 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 20000: 1.519745 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "qnly that attendegree colphitissia never unbergers of gbve forks  alivantatom siv\n",
      "jg one seven n five eight whis independents code in the words usually launch his \n",
      "txan sick is secrent prohiiasalhes system america said then manyyably circle for \n",
      "utes abrocusted for the valasiw sufji in don romanied is remays a several unlate \n",
      "wfs two then engages fer had on who for engergedetation and a most step of pherso\n",
      "================================================================================\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 20100: 1.525186 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 20200: 1.520671 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 20300: 1.552214 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 20400: 1.548169 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 20500: 1.545293 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 20600: 1.513473 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 20700: 1.506341 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 20800: 1.517141 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 20900: 1.518940 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 21000: 1.516408 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "jkgreat two seven famous the first examples supply for its louiws is interied tha\n",
      "hcs and s singreak united known bill feedel regording american accept unies of so\n",
      "rhanning however theory like foris especified such as a newspage well passes itst\n",
      "ae diff gift systems not of first part o thus on the great of a sola the gandedie\n",
      "rq envirses it of ibilitaul one nine th and influence the descriptiveyly to commu\n",
      "================================================================================\n",
      "Validation set perplexity: 6.39\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2)\n",
    "                    for _ in range(2):\n",
    "                        feed.append(random_distribution())\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({\n",
    "                            sample_input[0]: feed[0],\n",
    "                            sample_input[1]: feed[1],\n",
    "                        })\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1],\n",
    "                    keep_prob_sample: 1.0\n",
    "                })\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing Dropout in the LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal(\n",
    "        [embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal(\n",
    "        [embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.\n",
    "    cx = tf.Variable(tf.truncated_normal(\n",
    "        [embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal(\n",
    "        [embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_chars = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[2:]\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        bigram_index = tf.argmax(i[0], axis=1) + \\\n",
    "            vocabulary_size * tf.argmax(i[1], axis=1)\n",
    "        i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "        drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "        output, state = lstm_cell(drop_i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(axis=0, values=outputs), w, b)\n",
    "        drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(axis=0, values=train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 15000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    keep_prob_sample = tf.placeholder(tf.float32)\n",
    "    sample_input = list()\n",
    "    for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(\n",
    "            tf.float32, shape=[1, vocabulary_size]))\n",
    "    samp_in_index = tf.argmax(\n",
    "        sample_input[0], axis=1) + vocabulary_size * tf.argmax(sample_input[1], axis=1)\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(\n",
    "        vocabulary_embeddings, samp_in_index)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2)                    \n",
    "                    for _ in range(2):\n",
    "                        feed.append(random_distribution())\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({\n",
    "                            sample_input[0]: feed[0],\n",
    "                            sample_input[1]: feed[1],\n",
    "                        })\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                    print('Sentence: ', sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            print('valid_size: ', valid_size)\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1],\n",
    "                    keep_prob_sample: 1.0\n",
    "                })\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a utility for probabiity disctribution and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distribution: [ 0.08192215  0.00858568  0.02007259  0.02326842  0.08370015  0.03233085\n",
      "  0.03482114  0.04786063  0.05115731  0.0470081   0.04312964  0.02920952\n",
      "  0.04899316  0.08340634  0.00887989  0.07267074  0.08039548  0.07684735\n",
      "  0.03643272  0.08930814]\n",
      "Random Sample: [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "class ProbabilityUtil:\n",
    "    @staticmethod\n",
    "    def logprob(predictions, labels):\n",
    "        \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "        predictions[predictions < 1e-10] = 1e-10\n",
    "        return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def sample_distribution(distribution):\n",
    "        \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "        probabilities.\n",
    "        \"\"\"\n",
    "        r = random.uniform(0, 1)\n",
    "        s = 0\n",
    "        for i in range(len(distribution)):\n",
    "            s += distribution[i]\n",
    "            if s >= r:\n",
    "                return i\n",
    "        return len(distribution) - 1\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(prediction, vocabulary_size):\n",
    "        \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "        p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "        p[0, ProbabilityUtil.sample_distribution(prediction)] = 1.0\n",
    "        return p[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def random_distribution(vocabulary_size):\n",
    "        \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "        b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "        dist = b/np.sum(b, 1)[:,None]\n",
    "        return dist[0]\n",
    "    \n",
    "print('Random distribution:', ProbabilityUtil.random_distribution(20))\n",
    "print('Random Sample:', ProbabilityUtil.sample(ProbabilityUtil.random_distribution(20), 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use alphabets as our basic unit of information and keep track of the words and their ID in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence of \" the elimin\" translated to: [0, 22, 10, 7, 0, 7, 14, 11, 15, 11, 16]\n",
      "[85, 25, 36, 16, 27, 1, 2, 68, 31, 42, 37, 59, 81, 12, 91, 23, 0]  will translate to \"# w # n y . # # # # # # # j # u  \"\n",
      "Joining sentences looks like this: \"sentence to join .valid text.\"\n"
     ]
    }
   ],
   "source": [
    "class CharacterDictionary:\n",
    "    \"\"\"Stores all the english alphabet characters in a dictionary along with \n",
    "    known non-alphabet characters '.', '#', and ' '.\n",
    "    \"\"\"\n",
    "    END = '.'\n",
    "    UNK = '#'\n",
    "    PAD = ' '\n",
    "    def __init__(self):\n",
    "        self._dictionary = { self.PAD: 0, self.END: 1, self.UNK: 2 }\n",
    "        vocabulary_size = len(string.ascii_lowercase) + 3 # [a-z] + '.' + '#' + ' '\n",
    "        for char in string.ascii_lowercase:\n",
    "            self._dictionary[char.lower()] = len(self._dictionary)\n",
    "\n",
    "    def sentence2ids(self, sentence):\n",
    "        return [self.char2id(word) for word in sentence]\n",
    "    \n",
    "    def ids2sentence(self, sentence_ids):\n",
    "        return ' '.join([self.id2char(word_id) for word_id in sentence_ids])\n",
    "\n",
    "    def char2id(self, word):\n",
    "        if self.hasChar(word):\n",
    "            return self._dictionary[word]\n",
    "        else:\n",
    "            return self._dictionary[self.UNK]\n",
    "\n",
    "    def id2char(self, word_id):\n",
    "        if not self.hasId(word_id):\n",
    "            return self.UNK\n",
    "        else:\n",
    "            return self._dictionary.keys()[self._dictionary.values().index(word_id)]\n",
    "                \n",
    "    def hasChar(self, word):\n",
    "        return word in self._dictionary.keys()\n",
    "    \n",
    "    def hasId(self, word_id):\n",
    "        return word_id in self._dictionary.values()\n",
    "\n",
    "    def isEnd(self, word):\n",
    "        return word == self.END\n",
    "    \n",
    "    def idIsEnd(self, word_id):\n",
    "        return word_id == self._dictionary[self.END]\n",
    "        \n",
    "    def idIsUnknown(self, word_id):\n",
    "        return word_id == self._dictionary[self.UNK]\n",
    "        \n",
    "    def idIsPadding(self, word_id):\n",
    "        return word_id == self._dictionary[self.PAD]\n",
    "    \n",
    "    def length(self):\n",
    "        return len(self._dictionary)\n",
    "    \n",
    "    @staticmethod\n",
    "    def join(sentences):\n",
    "        \"\"\"Given an _array_ of strings we should join both sentences as one with an '<END>'\n",
    "        label inbetween the strings and a '<PAD>' tag at the end of both sentences.\n",
    "        \"\"\"\n",
    "        return ''.join([CharacterDictionary.END.join(sentences), CharacterDictionary.END])\n",
    "\n",
    "random_int = np.random.randint(0, len(valid_text))\n",
    "test_string =  valid_text[random_int:random_int + np.random.randint(10, 20)]\n",
    "test_array = [85, 25, 36, 16, 27, 1, 2, 68, 31, 42, 37, 59, 81, 12, 91, 23, 0]\n",
    "    \n",
    "print('Sentence of \"%s\" translated to:' % test_string, CharacterDictionary().sentence2ids(test_string))\n",
    "print(test_array, ' will translate to \"%s\"' % CharacterDictionary().ids2sentence(test_array))\n",
    "print('Joining sentences looks like this: \"%s\"' % CharacterDictionary.join(['sentence to join ', 'valid text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence \"ons anarchists advocate social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society however ideas about how an anarchist society might \" is mirrored to: \"([['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals.', 'fo suomonotua slaudividni.'], ['mutual aid and self governance.', 'lautum dia dna fles ecnanrevog.'], ['while anarchism is most easily.', 'elihw msihcrana si tsom ylisae.'], ['defined by what it is against anarchists.', 'denifed yb tahw ti si tsniaga stsihcrana.'], ['also offer positive visions.', 'osla reffo evitisop snoisiv.'], ['of what they believe to be a truly.', 'fo tahw yeht eveileb ot eb a ylurt.'], ['free society however ideas.', 'eerf yteicos revewoh saedi.'], ['about how an anarchist society.', 'tuoba woh na tsihcrana yteicos.'], ['might.', 'thgim.']], 41)\"\n",
      "Sample sentence \" This is some string \" is mirrored to: \"([[' This is some.', ' sihT si emos.'], ['string.', 'gnirts.']], 14)\"\n"
     ]
    }
   ],
   "source": [
    "class WordFlipper:\n",
    "    @staticmethod\n",
    "    def flip_word(word):\n",
    "        \"\"\" Just flip the characters in this given word\"\"\"\n",
    "        return word[::-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def flip(input_sentence):\n",
    "        flipped_sentence = ''\n",
    "        word_to_flip = ''\n",
    "        max_size = len(input_sentence) - 1\n",
    "        for index, char in enumerate(input_sentence):\n",
    "            if char == ' ' or index == max_size:\n",
    "                if index == max_size and char != ' ':\n",
    "                    flipped_sentence = ''.join([flipped_sentence, WordFlipper.flip_word(''.join([word_to_flip, char]))])\n",
    "                else:\n",
    "                    flipped_sentence =  ''.join([flipped_sentence, WordFlipper.flip_word(word_to_flip), char])\n",
    "                word_to_flip = ''\n",
    "            else:\n",
    "                word_to_flip += char\n",
    "        return flipped_sentence\n",
    "\n",
    "class MirroredCorpus:    \n",
    "    @staticmethod\n",
    "    def create(unflipped_text, min_chars = 20, max_chars = 40):\n",
    "        \"\"\" Given a corpus string of text I want to get random substets of sentences \n",
    "        and pair them with a sentence made up of a mirror of all its words.\n",
    "        \"\"\"\n",
    "        max_char_count = 0\n",
    "        flipped_text = ''\n",
    "        word_to_flip = ''  \n",
    "        string_to_flip = ''\n",
    "        char_count = 0\n",
    "        cursor_index = 0\n",
    "        random_int = np.random.randint(min_chars, max_chars)\n",
    "        flipped_text_pairs = []\n",
    "        for char in unflipped_text:\n",
    "            char_count += 1\n",
    "            if char_count >= random_int and char == ' ' or cursor_index == (len(unflipped_text) - 1):\n",
    "                if len(string_to_flip) > 0:\n",
    "                    flipped_text = CharacterDictionary.join([WordFlipper.flip(string_to_flip)])\n",
    "                    flipped_text_pairs.append([CharacterDictionary.join([string_to_flip]), flipped_text])\n",
    "                string_to_flip = ''\n",
    "                char_count = 0\n",
    "                random_int = np.random.randint(min_chars, max_chars)\n",
    "            else:\n",
    "                flipped_text = ''.join([flipped_text, char])\n",
    "                string_to_flip = ''.join([string_to_flip, char])\n",
    "            max_char_count = max(max_char_count, (char_count + 1)) # char_count includes dot\n",
    "            cursor_index += 1\n",
    "        return flipped_text_pairs, max_char_count # return the flipped text and the max number of characters\n",
    "\n",
    "test_string =  train_text[:330]\n",
    "\n",
    "print('Sample sentence \"%s\" is mirrored to: \"%s\"' %(test_string,  MirroredCorpus.create(test_string)))\n",
    "print('Sample sentence \"%s\" is mirrored to: \"%s\"' %(' This is some string ',  MirroredCorpus.create(' This is some string ', 0, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_train_char: \"41\"\n",
      "max_valid_char: \"39\"\n",
      "train_text[:300]: \"[['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .']]\"\n",
      "flip_train_text[:300]: \"[['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .']]\"\n",
      "--------------------------------------------------\n",
      "Printing test batches:\n",
      "[['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .'], ['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .'], ['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .'], ['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .'], ['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.']]\n",
      "[['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .'], ['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .'], ['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .'], ['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .'], ['ons anarchists advocate social relations.', 'sno stsihcrana etacovda laicos snoitaler.'], ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.'], ['of autonomous individuals mutual.', 'fo suomonotua slaudividni lautum.'], ['aid and self governance while anarchism.', 'dia dna fles ecnanrevog elihw msihcrana.'], ['is most easily defined by what it is.', 'si tsom ylisae denifed yb tahw ti si.'], ['against anarchists also offer positive.', 'tsniaga stsihcrana osla reffo evitisop.'], ['visions of what they believe to.', 'snoisiv fo tahw yeht eveileb ot.'], ['be a truly free society.', 'eb a ylurt eerf yteicos.'], ['however ideas about .', 'revewoh saedi tuoba .']]\n",
      "Single batch: ['based upon voluntary association.', 'desab nopu yratnulov noitaicossa.']\n",
      "--------------------------------------------------\n",
      "[[' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.']]\n",
      "[['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.'], ['class radicals including.', 'ssalc slacidar gnidulcni.'], [' anarchism originated as a term of.', ' msihcrana detanigiro sa a mret fo.'], ['abuse first used against early working.', 'esuba tsrif desu tsniaga ylrae gnikrow.']]\n",
      "Single batch: ['class radicals including.', 'ssalc slacidar gnidulcni.']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 30\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "class SequenceBatchGenerator:\n",
    "    def __init__(self, data, batch_size, num_unrollings=30):        \n",
    "        self._dictionary = CharacterDictionary()\n",
    "        self._data = data\n",
    "        self._data_size = len(self._data)\n",
    "        self._batch_size = batch_size       \n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._vocabulary_size = self._dictionary.length()\n",
    "        self._cursor = 0\n",
    "        self._last_batch = []\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\n",
    "        Ensure a batch is a complete sentence with both input and ouput string.\n",
    "        An input and output string must have '<END>' label at the end.\n",
    "        batch: [batch_size, [input: [vocab_size]], [output: [vocab_size]] ]\n",
    "        \"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, 2, self._vocabulary_size), dtype=np.float)\n",
    "        item_index = 0;\n",
    "        for batch_index in range(self._batch_size):\n",
    "            char_in = self._data[self._cursor % self._data_size][0][item_index % len(self._data[self._cursor % self._data_size][0])]\n",
    "            char_out = self._data[self._cursor % self._data_size][1][item_index % len(self._data[self._cursor % self._data_size][1])]            \n",
    "            item_index += 1\n",
    "            batch[batch_index, 0, self._dictionary.char2id(char_in)] = 1.0\n",
    "            batch[batch_index, 1, self._dictionary.char2id(char_out)] = 1.0\n",
    "            if self._dictionary.isEnd(char_in):\n",
    "                self._cursor  += 1\n",
    "                break\n",
    "        return batch\n",
    "        \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = []\n",
    "        if len(self._last_batch):\n",
    "            batches = [self._last_batch]            \n",
    "        for _ in range(self._batch_size - len(batches)):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "    \n",
    "    def decode_item(self, hot_encoding):\n",
    "        return self._dictionary.id2char(np.argmax(hot_encoding))\n",
    "    \n",
    "    def decode_batch(self, batch, single=False):\n",
    "        \"\"\"Turn all 1-hot encoding or all probability distribution over the possible\n",
    "        string back into its (most likely) words representation.\"\"\"\n",
    "        string, string_in, string_out = [],[],[]\n",
    "        sentences = []\n",
    "        for hot_encoding in batch:\n",
    "            if not single:\n",
    "                char_in = self.decode_item(hot_encoding[0])\n",
    "                char_out = self.decode_item(hot_encoding[1])\n",
    "                string_in.append(char_in)\n",
    "                string_out.append(char_out)\n",
    "                if self._dictionary.isEnd(char_in):\n",
    "                    string_in = ''.join(string_in)\n",
    "                    string_out = ''.join(string_out)                   \n",
    "                    sentences = [string_in, string_out]\n",
    "                    string_in, string_out = [],[]\n",
    "            else:\n",
    "                char = self.decode_item(hot_encoding)\n",
    "                string.append(char)\n",
    "                if self._dictionary.isEnd(char):\n",
    "                    sentences = ''.join(string)\n",
    "                    string = []\n",
    "        return sentences\n",
    "    \n",
    "    def decode_batches(self, batches, single=False):\n",
    "        \"\"\"Convert a sequence of batches back into their (most likely) sentence\n",
    "        representations.\"\"\"\n",
    "        if not single:\n",
    "            return [self.decode_batch(batch)\n",
    "                for batch in batches ]\n",
    "        else:\n",
    "            return [self.decode_batch(batch, single)\n",
    "                for batch in batches ]\n",
    "    \n",
    "flip_train_data, max_train_char = MirroredCorpus.create(train_text[:300])\n",
    "flip_valid_data, max_valid_char = MirroredCorpus.create(valid_text[:100])\n",
    "train_sequence_batches = SequenceBatchGenerator(flip_train_data, max_train_char)\n",
    "valid_sequence_batches = SequenceBatchGenerator(flip_valid_data, max_valid_char)\n",
    "\n",
    "print('max_train_char: \"%s\"'% max_train_char)\n",
    "print('max_valid_char: \"%s\"'% max_valid_char)\n",
    "print('train_text[:300]: \"%s\"'% flip_train_data[:30])\n",
    "print('flip_train_text[:300]: \"%s\"'% flip_train_data[:])\n",
    "print('-'*50)\n",
    "print('Printing test batches:')\n",
    "print(train_sequence_batches.decode_batches(train_sequence_batches.next()))\n",
    "print(train_sequence_batches.decode_batches(train_sequence_batches.next()))\n",
    "print('Single batch:', train_sequence_batches.decode_batch(train_sequence_batches.next()[2]))\n",
    "print('-'*50)\n",
    "print(valid_sequence_batches.decode_batches(valid_sequence_batches.next()))\n",
    "print(valid_sequence_batches.decode_batches(valid_sequence_batches.next()))\n",
    "print('Single batch:', valid_sequence_batches.decode_batch(valid_sequence_batches.next()[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build out our the LSTM Cell with a better understanding of its gate base structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTMCell(x, m_prev, c_prev, cell_weigths, cell_biases):\n",
    "    \"\"\"Create a LSTM cell that takes in the current input and previous\n",
    "    output (if any) to generate a cell output and state\n",
    "    References:\n",
    "        - http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        - https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "    Parameters:\n",
    "        - x: input for the cell\n",
    "        - m_prev: output of the previous cell unrolling\n",
    "        - c_prev: cell state of the previous cell unrolling\n",
    "        - cell_weights: weights for all gates in the cell \n",
    "        - cell_biases: biases for all gates in the cell\n",
    "    Notes:\n",
    "        - w represents the weight matrices (e.g. w_ix is the matrix\n",
    "        of weights from the input gate to the input)\n",
    "        - the b terms denote bias vectors (b_i is the input gate bias vector)\n",
    "        - the labels i, f, o, c and m are respectively vectors from the input gate, \n",
    "        forget gate, output gate, cell activation and output activation\n",
    "        - c is also known as the cell state\n",
    "        - m can also be known as the cell output sometimes denoted by h\n",
    "        - elements from previous vectors are postfixed with _prev\n",
    "        - cell_weights: [[w_ix, w_im, w_ic], [w_fx, w_fm, w_fc], [w_ox, w_om, w_oc], [w_cx, w_cm]]\n",
    "        - cell_biases: [b_i, b_f, b_o, b_c]\n",
    "    \"\"\"\n",
    "    # Get LSTM Cell features (parameters)\n",
    "    b_i, b_f, b_o, b_c = cell_biases\n",
    "    w_i, w_f, w_o, w_c = cell_weigths\n",
    "    w_ix, w_im, w_ic = w_i\n",
    "    w_fx, w_fm, w_fc = w_f\n",
    "    w_ox, w_om, w_oc = w_o\n",
    "    w_cx, w_cm = w_c\n",
    "    # Create LSTM gates\n",
    "    i_gate = tf.sigmoid(tf.matmul(x, w_ix) + tf.matmul(m_prev, w_im) + \n",
    "                        tf.matmul(c_prev, w_ic) + b_i)\n",
    "    f_gate = tf.sigmoid(tf.matmul(x, w_fx) + tf.matmul(m_prev, w_fm) +\n",
    "                        tf.matmul(c_prev, w_fc) + b_f)\n",
    "    c_tanh = tf.tanh(tf.matmul(x, w_cx) + tf.matmul(m_prev, w_cm) + b_c)\n",
    "    c_gate = f_gate * c_prev + i_gate * c_tanh\n",
    "    o_gate = tf.sigmoid(tf.matmul(x, w_ox) + tf.matmul(m_prev, w_om) +\n",
    "                        tf.matmul(c_prev, w_oc) + b_o)\n",
    "    m_gate = o_gate * tf.tanh(c_gate)\n",
    "    return m_gate, c_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we go on to building our encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, options):\n",
    "        \"\"\"Create a number of LSTM and prepare them for training to predict the letter\n",
    "        that comes after a sequence of characters. The output will be used by and decoder.\n",
    "        Using the many-to-one architecture for variable length inputs.\n",
    "        \"\"\"\n",
    "        self._graph = options['graph']\n",
    "        self._cell_size = options['cell_size']\n",
    "        self._cell_layers = options['cell_layers']\n",
    "        self._batch_size = options['batch_size']\n",
    "        self._char_size = options['char_size']\n",
    "        self._num_nodes = options['num_nodes']\n",
    "        self._embedding_size = options['embedding_size']\n",
    "        self._vocabulary_size = options['vocabulary_size']\n",
    "        self._num_unrollings = options['num_unrollings']\n",
    "        self._vocabulary_embeddings = options['vocabulary_embeddings']\n",
    "\n",
    "        self._loss = 0\n",
    "        self._global_step = 0\n",
    "        self._learning_rate = 0\n",
    "        self._logits = None\n",
    "        self._optimizer = None\n",
    "        self._train_prediction = None\n",
    "        self._sample_prediction = None\n",
    "        self._weights = []\n",
    "        self._biases = []\n",
    "        self._cell_weights = []\n",
    "        self._cell_biases = []\n",
    "        \n",
    "        self._train_input = []\n",
    "        self._train_output = []\n",
    "        self._sample_state = []\n",
    "        self._sample_input = []\n",
    "        self._sample_output = []\n",
    "        self._relevant_state = None\n",
    "        self._relevant_output = None\n",
    "        self._reset_sample_state = None\n",
    "        \n",
    "        self._scope = None\n",
    "        self._session = None\n",
    "        self._variable_initializer = None\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        with tf.variable_scope(\"encoder\") as self._scope:\n",
    "            # Variables saving state across unrollings.\n",
    "            saved_output =  tf.get_variable(\"saved_outut\", shape=[self._batch_size, self._num_nodes], initializer=tf.zeros_initializer, trainable=False)\n",
    "            saved_state = tf.Variable(tf.zeros([self._batch_size, self._num_nodes]), trainable=False)\n",
    "            # Classifier weights and biases.\n",
    "            self._weights = tf.Variable(tf.truncated_normal([self._num_nodes, self._vocabulary_size], -0.1, 0.1))\n",
    "            self._biases = tf.Variable(tf.zeros([self._vocabulary_size]))\n",
    "            # Variables for the LSTM Cell\n",
    "            # Input gate weights\n",
    "            w_ix = tf.Variable(tf.truncated_normal([self._embedding_size, self._num_nodes], -0.1, 0.1))\n",
    "            w_im = tf.Variable(tf.truncated_normal([self._num_nodes, self._num_nodes], -0.1, 0.1))\n",
    "            w_ic = tf.Variable(tf.truncated_normal([self._num_nodes, self._num_nodes], -0.1, 0.1))\n",
    "            # Forget gate weights\n",
    "            w_fx = tf.Variable(tf.truncated_normal([self._embedding_size, self._num_nodes], -0.1, 0.1))\n",
    "            w_fm = tf.Variable(tf.truncated_normal([self._num_nodes, self._num_nodes], -0.1, 0.1))\n",
    "            w_fc = tf.Variable(tf.truncated_normal([self._num_nodes, self._num_nodes], -0.1, 0.1))\n",
    "            # Output gate weights\n",
    "            w_ox = tf.Variable(tf.truncated_normal([self._embedding_size, self._num_nodes], -0.1, 0.1))\n",
    "            w_om = tf.Variable(tf.truncated_normal([self._num_nodes, self._num_nodes], -0.1, 0.1))\n",
    "            w_oc = tf.Variable(tf.truncated_normal([self._num_nodes, self._num_nodes], -0.1, 0.1))\n",
    "            # State gate\n",
    "            w_cx = tf.Variable(tf.truncated_normal([self._embedding_size, self._num_nodes], -0.1, 0.1))\n",
    "            w_cm = tf.Variable(tf.truncated_normal([self._num_nodes, self._num_nodes], -0.1, 0.1))\n",
    "            # Biases\n",
    "            b_i = tf.Variable(tf.zeros([1, self._num_nodes]))\n",
    "            b_f = tf.Variable(tf.zeros([1, self._num_nodes]))\n",
    "            b_o = tf.Variable(tf.zeros([1, self._num_nodes]))\n",
    "            b_c = tf.Variable(tf.zeros([1, self._num_nodes]))\n",
    "            # Concatenate Varaiables so we can pass them to the LSTM cell easily\n",
    "            self._cell_weights = [[w_ix, w_im, w_ic], [w_fx, w_fm, w_fc], [w_ox, w_om, w_oc], [w_cx, w_cm]]\n",
    "            self._cell_biases = [b_i, b_f, b_o, b_c]\n",
    "            \n",
    "            # Input data.\n",
    "            self._train_input = list()\n",
    "            for _ in range(self._num_unrollings):\n",
    "                self._train_input.append( \n",
    "                    tf.placeholder(tf.float32, shape=[self._batch_size, self._vocabulary_size]))\n",
    "            # Change output to be next character in sequence\n",
    "            # Outputs (labels) of the encoder are inputs at the next time step.\n",
    "            self._train_output = tf.placeholder(tf.float32, shape=[self._batch_size, self._vocabulary_size])\n",
    "            \n",
    "            # Unrolled LSTM loop.\n",
    "            self._outputs = []\n",
    "            output = saved_output\n",
    "            state = saved_state\n",
    "            for i in self._train_input:\n",
    "                i_embed = tf.nn.embedding_lookup(self._vocabulary_embeddings, tf.argmax(i, axis=1))\n",
    "                output, state = LSTMCell(i_embed, output, state, self._cell_weights, self._cell_biases)\n",
    "                self._outputs.append(output)\n",
    "            \n",
    "            # The relevant output is at the last time step\n",
    "            self._relevant_output = output\n",
    "            self._relevant_state = state\n",
    "                \n",
    "            # State saving across unrollings.\n",
    "            with tf.control_dependencies([saved_output.assign(self._relevant_output),\n",
    "                saved_state.assign(self._relevant_state)]):\n",
    "                # Classifier.\n",
    "                self._logits = tf.nn.xw_plus_b(output, self._weights, self._biases)\n",
    "                self._loss = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        labels=self._train_output, logits=self._logits))\n",
    "\n",
    "            # Optimizer.\n",
    "            self._global_step = tf.Variable(0)\n",
    "            self._learning_rate = tf.train.exponential_decay(\n",
    "                10.0, self._global_step, 5000, 0.1, staircase=True)\n",
    "            self._optimizer = tf.train.GradientDescentOptimizer(self._learning_rate)\n",
    "            gradients, v = zip(*self._optimizer.compute_gradients(self._loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "            self._optimizer = self._optimizer.apply_gradients(\n",
    "                zip(gradients, v), global_step=self._global_step)\n",
    "\n",
    "            # Predictions.\n",
    "            self._train_prediction = tf.nn.softmax(self._logits)\n",
    "            \n",
    "            # Validate \n",
    "            self._validate()\n",
    "            \n",
    "            # Variable Initializer for the Encoder scope\n",
    "            self._variable_initializer = tf.global_variables_initializer()\n",
    "\n",
    "                \n",
    "    def _validate(self, batch_size=1):\n",
    "        \"\"\"Sampling and validation evaluation with a batch size of one and no unrolling.\n",
    "        \"\"\"\n",
    "        self._sample_input = tf.placeholder(tf.float32, shape=[1, self._vocabulary_size])\n",
    "        sample_input_embedding = tf.nn.embedding_lookup(self._vocabulary_embeddings, tf.argmax(self._sample_input, axis=1))\n",
    "        saved_sample_output = tf.Variable(tf.zeros([1, self._num_nodes]))\n",
    "        saved_sample_state = tf.Variable(tf.zeros([1, self._num_nodes]))\n",
    "        self._reset_sample_state = tf.group( \n",
    "            saved_sample_output.assign(tf.zeros([1, self._num_nodes])),\n",
    "            saved_sample_state.assign(tf.zeros([1, self._num_nodes])))\n",
    "        self._sample_output, self._sample_state = LSTMCell( \n",
    "            sample_input_embedding, saved_sample_output, saved_sample_state, self._cell_weights, self._cell_biases)\n",
    "        with tf.control_dependencies([saved_sample_output.assign(self._sample_output),\n",
    "            saved_sample_state.assign(self._sample_state)]):\n",
    "            self._sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(self._sample_output, self._weights, self._biases))\n",
    "    \n",
    "    def _train(self, sequence_batches, num_steps = 7001, summary_frequency = 200, valid_sequence_batches = None):\n",
    "        \"\"\"Train the encoder model with our batched data\n",
    "        \"\"\" \n",
    "        print('Training the sequence encoder!')\n",
    "        with tf.Session(graph=self._graph) as self._session:\n",
    "            assert self._session.graph is self._graph\n",
    "            self._session.run(self._variable_initializer)\n",
    "            mean_loss = 0\n",
    "            for step in range(num_steps):\n",
    "                batches = sequence_batches.next()\n",
    "                feed_dict = dict()\n",
    "                for i in range(self._num_unrollings):\n",
    "                    feed_dict[self._train_input[i]] = np.array([b[0] for b in batches[i]])\n",
    "                ## The output feed (label) is the first character of the output string\n",
    "                feed_dict[self._train_output] = np.array([batch[1][0] for batch in batches])\n",
    "                _, l, predictions, lr = self._session.run(\n",
    "                    [self._optimizer, self._loss, self._train_prediction, self._learning_rate], \n",
    "                    feed_dict=feed_dict)\n",
    "                mean_loss += l\n",
    "                if step % summary_frequency == 0:\n",
    "                    if step > 0:\n",
    "                        mean_loss = mean_loss / summary_frequency\n",
    "                    # The mean loss is an estimate of the loss over the last few batches.\n",
    "                    print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                    mean_loss = 0\n",
    "                    labels = feed_dict[self._train_output]\n",
    "                    print('Minibatch perplexity: %.2f' % float(\n",
    "                        np.exp(ProbabilityUtil.logprob(predictions, labels))))\n",
    "                    if step % (summary_frequency * 10) == 0:\n",
    "                        # Generate some single one-hot encoding samples.\n",
    "                        print('=' * 80)\n",
    "                        input_feed, output_feed = [],[]\n",
    "                        input_chars, output_chars = '',''\n",
    "                        ## We should feed the model one character at a time and take the last prediction\n",
    "                        for _ in range(self._batch_size):\n",
    "                            item = ProbabilityUtil.sample(ProbabilityUtil.random_distribution(self._vocabulary_size), self._vocabulary_size)                            \n",
    "                            item = item.reshape([1, self._vocabulary_size])\n",
    "                            input_feed.append(item)\n",
    "                            self._reset_sample_state.run()\n",
    "                            prediction = self._sample_prediction.eval(\n",
    "                                {self._sample_input: item})\n",
    "                            out_item = ProbabilityUtil.sample(prediction[0], self._vocabulary_size)\n",
    "                            output_feed.append(out_item)\n",
    "                            input_chars += sequence_batches.decode_item(item)\n",
    "                            output_chars += sequence_batches.decode_item(out_item)\n",
    "                        print('For input sentence \"%s\" we got an encoder output of \"%s\"' %(\n",
    "                            input_chars, \n",
    "                            output_chars))\n",
    "                        # NOTE: Okay, we are only able to predict one char at a time. This is not enough\n",
    "                        print('=' * 80)\n",
    "                    # Measure validation set perplexity.\n",
    "                    self._reset_sample_state.run()\n",
    "                    if valid_sequence_batches:\n",
    "                        valid_logprob = 0\n",
    "                        for _ in range(valid_size):\n",
    "                            b = valid_sequence_batches.next()                                         \n",
    "                            bv = np.array([ ba[1] for ba in b[i]])\n",
    "                            predictions = self._sample_prediction.eval({self._sample_input: bv})\n",
    "                            valid_logprob = valid_logprob + ProbabilityUtil.logprob(predictions, b[1])\n",
    "                        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                            valid_logprob / valid_size)))\n",
    "        print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an encoder we of course need our decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, cell_size, cell_layers=1):\n",
    "        \"\"\"Create a number of LSTM and prepare them for training to predict the next letter\n",
    "        given a sequence of characters. Beam serach can also be used to improve prediction.\n",
    "        \"\"\"\n",
    "        self._cell_size = cell_size\n",
    "        self._cell_layers = cell_layers\n",
    "        pass\n",
    "    \n",
    "    def _train(self, sequence_batches, num_steps = 7001, summary_frequency = 100):\n",
    "        print('Training the sequence decoder!')\n",
    "        pass     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then need to build a sequence-to-sequence model for our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SequenceModel:\n",
    "    def __init__(self, options):\n",
    "        self._options = options\n",
    "        self._encoder = None\n",
    "        self._decoder = None\n",
    "        # Create the Tensor graph\n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._vocabulary_embeddings = tf.Variable( \n",
    "                tf.random_uniform([self._options['vocabulary_size'], \n",
    "                                   self._options['embedding_size']], -1.0, 1.0))\n",
    "            options['vocabulary_embeddings'] = self._vocabulary_embeddings\n",
    "            options['graph'] = self._graph\n",
    "            self._encoder = self._build_encoder(self._options)\n",
    "            self._decoder = self._build_decoder(self._options)\n",
    "    \n",
    "    def _build_encoder(self, options):\n",
    "        \"\"\"Build the encoder network using its own personal variable scope\n",
    "        \"\"\"\n",
    "        return Encoder(options)\n",
    "            \n",
    "    def _build_decoder(self, options):\n",
    "        \"\"\"Build the decoder network using its own personal variable scope\n",
    "        \"\"\"\n",
    "        return Decoder(options)\n",
    "    \n",
    "    def train(self, sequence_batches):\n",
    "        \"\"\"We would like to train our encode to predict a first letter given some state.\n",
    "        Then we will also train our decoder to predict the next character given.\n",
    "        Training data contains batches with arrays of both input and output sequences.\n",
    "        Takes the training sequence of batches.\n",
    "        \"\"\"\n",
    "        self._encoder._train(sequence_batches)        \n",
    "        self._decoder._train(sequence_batches)\n",
    "    \n",
    "    def validate(self):\n",
    "        self._encoder._validate()\n",
    "        self._decoder._validate()        \n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the model to predict a sequence of words that ends with `<EOS>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the sequence encoder!\n",
      "Average loss at step 0: 3.438741 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.15\n",
      "================================================================================\n",
      "For input sentence \"cwxqjj fsnyaxhseqyga.lpjjtukrzgmaeo\" we got an encoder output of \"le.fdoosseoeoesoftgeooffoazeshkgoet\"\n",
      "================================================================================\n",
      "Average loss at step 200: 3.149766 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.90\n",
      "Average loss at step 400: 3.092173 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.24\n",
      "Average loss at step 600: 3.148765 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.92\n",
      "Average loss at step 800: 3.150891 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.04\n",
      "Average loss at step 1000: 3.092864 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.03\n",
      "Average loss at step 1200: 2.990771 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.17\n",
      "Average loss at step 1400: 3.078714 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.76\n",
      "Average loss at step 1600: 2.987751 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.04\n",
      "Average loss at step 1800: 3.003209 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.19\n",
      "Average loss at step 2000: 2.940013 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.44\n",
      "================================================================================\n",
      "For input sentence \"p wcj#nrhvguhosluwzqgjkd.dnfw.vhrce\" we got an encoder output of \"efennnnghnnhnfnsnnnsnnenfenhnhnhggn\"\n",
      "================================================================================\n",
      "Average loss at step 2200: 2.924972 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.25\n",
      "Average loss at step 2400: 3.043766 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.00\n",
      "Average loss at step 2600: 3.041581 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.55\n",
      "Average loss at step 2800: 2.994787 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.74\n",
      "Average loss at step 3000: 3.005904 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.49\n",
      "Average loss at step 3200: 3.136109 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.72\n",
      "Average loss at step 3400: 3.042343 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.05\n",
      "Average loss at step 3600: 3.060939 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.24\n",
      "Average loss at step 3800: 2.978585 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.80\n",
      "Average loss at step 4000: 3.110189 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.59\n",
      "================================================================================\n",
      "For input sentence \"ehqiapb ixntd easppakpcnolwlfjtkxuu\" we got an encoder output of \"eseeheefeneeehgheeehseeeehneeeeheee\"\n",
      "================================================================================\n",
      "Average loss at step 4200: 2.995046 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.79\n",
      "Average loss at step 4400: 2.995484 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.94\n",
      "Average loss at step 4600: 2.854390 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.76\n",
      "Average loss at step 4800: 2.931974 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.98\n",
      "Average loss at step 5000: 2.984111 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.72\n",
      "Average loss at step 5200: 1.788144 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Average loss at step 5400: 1.775957 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Average loss at step 5600: 1.775869 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Average loss at step 5800: 1.775826 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Average loss at step 6000: 1.775800 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "================================================================================\n",
      "For input sentence \"eem.#a#cronrej.ulh#iauwmrmnlcszsz#o\" we got an encoder output of \"nnesehnenhesnnheeheshennnsehfennnee\"\n",
      "================================================================================\n",
      "Average loss at step 6200: 1.775781 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Average loss at step 6400: 1.775768 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Average loss at step 6600: 1.775758 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Average loss at step 6800: 1.775750 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Average loss at step 7000: 1.775743 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Training completed!\n",
      "Training the sequence decoder!\n",
      "Model created!\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 60\n",
    "batch_size = max_train_char # TODO we should not use maxiumum character count as the batch size\n",
    "vocabulary_size = train_sequence_batches._dictionary.length()\n",
    "max_char_count = max(max_train_char, max_valid_char)\n",
    "\n",
    "# Create options for our network\n",
    "model_options = {\n",
    "    'batch_size': batch_size, \n",
    "    'char_size': max_train_char, \n",
    "    'embedding_size': embedding_size, \n",
    "    'num_nodes': num_nodes, \n",
    "    'vocabulary_size': vocabulary_size, \n",
    "    'num_unrollings': num_unrollings,\n",
    "    'num_time_steps': num_unrollings,\n",
    "    'cell_size': 1,\n",
    "    'cell_layers': 1\n",
    "}\n",
    "sequence_model = SequenceModel(model_options)\n",
    "train_sequence_batches\n",
    "valid_sequence_batches\n",
    "sequence_model.train(train_sequence_batches)\n",
    "\n",
    "print('Model completed!')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
