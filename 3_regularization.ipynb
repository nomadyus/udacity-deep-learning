{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L2 Regularization for Logistic Model\n",
      "Tensorflow Graph created\n",
      "Initialized\n",
      "Minibatch loss at step 0: 24.617813\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 6.0%\n",
      "Minibatch loss at step 500: 2.330762\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1000: 1.741897\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 1500: 1.030663\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2000: 0.815896\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 2500: 0.812145\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 3000: 0.782411\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "print('Using L2 Regularization for Logistic Model')\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_l2_feature = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + tf_l2_feature * tf.nn.l2_loss(weights)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "print('Tensorflow Graph created')\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunning the L2 Regularization constant\n",
      "L2 Regularization constant of 0.000100 performed with 86.90% accuracy\n",
      "L2 Regularization constant of 0.000126 performed with 86.60% accuracy\n",
      "L2 Regularization constant of 0.000158 performed with 87.20% accuracy\n",
      "L2 Regularization constant of 0.000200 performed with 87.41% accuracy\n",
      "L2 Regularization constant of 0.000251 performed with 87.36% accuracy\n",
      "L2 Regularization constant of 0.000316 performed with 87.69% accuracy\n",
      "L2 Regularization constant of 0.000398 performed with 88.31% accuracy\n",
      "L2 Regularization constant of 0.000501 performed with 88.34% accuracy\n",
      "L2 Regularization constant of 0.000631 performed with 88.55% accuracy\n",
      "L2 Regularization constant of 0.000794 performed with 88.71% accuracy\n",
      "L2 Regularization constant of 0.001000 performed with 88.97% accuracy\n",
      "L2 Regularization constant of 0.001259 performed with 88.93% accuracy\n",
      "L2 Regularization constant of 0.001585 performed with 89.14% accuracy\n",
      "L2 Regularization constant of 0.001995 performed with 89.13% accuracy\n",
      "L2 Regularization constant of 0.002512 performed with 89.06% accuracy\n",
      "L2 Regularization constant of 0.003162 performed with 89.06% accuracy\n",
      "L2 Regularization constant of 0.003981 performed with 88.98% accuracy\n",
      "L2 Regularization constant of 0.005012 performed with 88.99% accuracy\n",
      "L2 Regularization constant of 0.006310 performed with 88.86% accuracy\n",
      "L2 Regularization constant of 0.007943 performed with 88.70% accuracy\n"
     ]
    }
   ],
   "source": [
    "print('Tunning the L2 Regularization constant')\n",
    "\n",
    "num_steps = 3001\n",
    "l2_constant_values = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_values = []\n",
    "max_accuracy, best_l2_constant = 0, 0\n",
    "\n",
    "for l2_constant in l2_constant_values:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: l2_constant}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "        if test_accuracy == max(max_accuracy, test_accuracy):\n",
    "            max_accuracy = test_accuracy\n",
    "            best_l2_constant = l2_constant\n",
    "        accuracy_values.append(test_accuracy)\n",
    "        print('L2 Regularization constant of %f performed with %.2f%% accuracy' % (l2_constant, accuracy_values[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot the L2 Regularization loss for our Test\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ac84b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum accuracy is 89.14% with L2 Regularization constant of 0.001585\n"
     ]
    }
   ],
   "source": [
    "print('Plot the L2 Regularization loss for our Test')\n",
    "plt.semilogx(l2_constant_values, accuracy_values)\n",
    "plt.grid(True)\n",
    "plt.title('Accuracy against L2 regularization (Logistic Regression)')\n",
    "plt.show()\n",
    "print('Maximum accuracy is %.2f%% with L2 Regularization constant of %f' % (max_accuracy, best_l2_constant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L2 Regularization for Neural Network Model (1 Layer)\n",
      "Tensorflow Graph created\n",
      "Initialized\n",
      "Minibatch loss at step 0: 600.963013\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 20.0%\n",
      "Minibatch loss at step 500: 201.086609\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 1000: 115.457634\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1500: 68.334595\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 2000: 41.258797\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2500: 25.166409\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 3000: 15.486179\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.1%\n",
      "Test accuracy: 92.8%\n"
     ]
    }
   ],
   "source": [
    "print('Using L2 Regularization for Neural Network Model (1 Layer)')\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "# Buildig the Network\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_l2_feature = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    layer1_biases = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    layer2_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    layer2_biases = tf.Variable(tf.zeros([num_labels]))    \n",
    "\n",
    "    # Training computation.\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, layer1_weights) + layer1_biases)\n",
    "    logits = tf.matmul(hidden_layer, layer2_weights) + layer2_biases\n",
    "    loss = tf.reduce_mean( \\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    tf_l2_feature * (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    hidden_layer_valid_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, layer1_weights) + layer1_biases)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_layer_valid_prediction, layer2_weights) + layer2_biases)\n",
    "    hidden_layer_test_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, layer1_weights) + layer1_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_layer_test_prediction, layer2_weights) + layer2_biases)\n",
    "\n",
    "print('Tensorflow Graph created')\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunning the L2 Regularization constant\n",
      "Accuracy of 89.71% for L2 parameter constant of 0.000100\n",
      "Accuracy of 89.42% for L2 parameter constant of 0.000126\n",
      "Accuracy of 89.43% for L2 parameter constant of 0.000158\n",
      "Accuracy of 90.34% for L2 parameter constant of 0.000200\n",
      "Accuracy of 90.70% for L2 parameter constant of 0.000251\n",
      "Accuracy of 90.78% for L2 parameter constant of 0.000316\n",
      "Accuracy of 90.94% for L2 parameter constant of 0.000398\n",
      "Accuracy of 91.20% for L2 parameter constant of 0.000501\n",
      "Accuracy of 92.10% for L2 parameter constant of 0.000631\n",
      "Accuracy of 92.58% for L2 parameter constant of 0.000794\n",
      "Accuracy of 93.20% for L2 parameter constant of 0.001000\n",
      "Accuracy of 93.22% for L2 parameter constant of 0.001259\n",
      "Accuracy of 93.27% for L2 parameter constant of 0.001585\n",
      "Accuracy of 92.98% for L2 parameter constant of 0.001995\n",
      "Accuracy of 92.63% for L2 parameter constant of 0.002512\n",
      "Accuracy of 92.40% for L2 parameter constant of 0.003162\n",
      "Accuracy of 92.09% for L2 parameter constant of 0.003981\n",
      "Accuracy of 91.75% for L2 parameter constant of 0.005012\n",
      "Accuracy of 91.33% for L2 parameter constant of 0.006310\n",
      "Accuracy of 90.91% for L2 parameter constant of 0.007943\n"
     ]
    }
   ],
   "source": [
    "print('Tunning the L2 Regularization constant')\n",
    "\n",
    "num_steps = 3001\n",
    "l2_constant_values = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_values = []\n",
    "max_accuracy, best_l2_constant = 0, 0\n",
    "\n",
    "for l2_constant in l2_constant_values:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: l2_constant}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "        if test_accuracy == max(accuracy_values, test_accuracy):\n",
    "            max_accuracy = test_accuracy\n",
    "            best_ls_constact = l2_constant\n",
    "        accuracy_values.append(test_accuracy)\n",
    "    print('Accuracy of %.2f%% for L2 parameter constant of %f' % (accuracy_values[-1], l2_constant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot the L2 Regularization loss for our Test\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEMCAYAAAAxoErWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmIAsSRhjjAno/wE9MRf61qhHPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ad4aed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum accuracy is 0.00% with L2 Regularization constant of 0.000000\n"
     ]
    }
   ],
   "source": [
    "print('Plot the L2 Regularization loss for our Test')\n",
    "plt.semilogx(l2_constant_values, accuracy_values)\n",
    "plt.grid(True)\n",
    "plt.title('Accuracy against L2 regularization (1 Layer Neural Network)')\n",
    "plt.show()\n",
    "print('Maximum accuracy is %.2f%% with L2 Regularization constant of %f' % (max_accuracy, best_l2_constant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L2 Regularization for Neural Network Model (1 Layer) with overfitting\n",
      "Tensorflow Graph created\n",
      "Initialized\n",
      "Minibatch loss at step 0: 331.357239\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 24.1%\n",
      "Minibatch loss at step 1: 1060.645874\n",
      "Minibatch accuracy: 23.4%\n",
      "Validation accuracy: 37.0%\n",
      "Minibatch loss at step 2: 1377.602539\n",
      "Minibatch accuracy: 44.5%\n",
      "Validation accuracy: 39.9%\n",
      "Minibatch loss at step 3: 1212.494019\n",
      "Minibatch accuracy: 28.1%\n",
      "Validation accuracy: 40.2%\n",
      "Minibatch loss at step 4: 431.445068\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 58.1%\n",
      "Minibatch loss at step 5: 289.389648\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 60.5%\n",
      "Minibatch loss at step 6: 255.189468\n",
      "Minibatch accuracy: 55.5%\n",
      "Validation accuracy: 71.6%\n",
      "Minibatch loss at step 7: 111.873390\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 8: 98.487244\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 9: 66.364189\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 10: 100.928520\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 11: 121.527985\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 12: 72.738358\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 13: 62.668262\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 14: 102.719803\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 15: 83.406784\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 16: 126.057816\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 17: 79.457184\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 18: 213.356262\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 19: 134.048950\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 20: 75.662315\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 21: 80.708038\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 22: 76.666611\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 23: 95.406570\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 24: 36.169884\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 25: 131.785019\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 26: 46.305496\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 27: 96.231140\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 28: 62.442009\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 29: 69.860046\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 30: 52.110050\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 31: 36.349571\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 32: 80.268867\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 33: 34.577129\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 34: 56.642868\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 35: 57.843163\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 36: 82.709496\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 37: 56.619331\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 38: 61.726471\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 39: 124.141739\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 40: 186.200974\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 41: 94.620834\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 42: 60.204071\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 74.3%\n",
      "Minibatch loss at step 43: 42.301430\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 44: 75.116882\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 45: 74.288925\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 65.8%\n",
      "Minibatch loss at step 46: 106.214081\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 47: 33.084206\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 48: 69.894730\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 49: 57.017181\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 50: 51.627354\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 51: 41.564110\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 52: 31.335045\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 53: 65.762161\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 54: 96.501831\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 73.1%\n",
      "Minibatch loss at step 55: 62.120708\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 56: 49.044167\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 57: 87.715019\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 58: 53.732018\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 59: 52.508507\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 60: 43.141754\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 61: 37.849770\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 62: 39.522884\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 63: 31.456125\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 64: 37.573818\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 65: 67.525757\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 66: 53.243843\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 67: 69.689499\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 68: 39.565746\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 69: 72.725029\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 70: 62.442532\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 71: 93.478996\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 72: 61.745693\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 73: 63.371117\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 74: 47.967411\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 75: 24.230042\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 76: 54.160522\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 77: 19.337507\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 78: 38.431961\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 79: 54.289589\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 80: 55.009834\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 81: 51.921780\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 82: 38.426182\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 83: 50.732224\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 84: 57.285542\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 85: 40.780273\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 86: 39.099525\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 87: 31.237000\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 88: 40.012123\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 89: 35.737411\n",
      "Minibatch accuracy: 78.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 90: 22.866604\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 91: 15.765753\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 92: 46.764633\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 93: 39.775341\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 94: 68.661163\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 95: 42.960827\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 96: 39.657455\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 97: 61.205605\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 98: 36.469139\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 99: 41.573650\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 100: 41.579025\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.9%\n",
      "Test accuracy: 84.1%\n"
     ]
    }
   ],
   "source": [
    "print('Using L2 Regularization for Neural Network Model (1 Layer) with overfitting')\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "# Buildig the Network\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_l2_feature = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    layer1_biases = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    layer2_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    layer2_biases = tf.Variable(tf.zeros([num_labels]))    \n",
    "\n",
    "    # Training computation.\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, layer1_weights) + layer1_biases)\n",
    "    logits = tf.matmul(hidden_layer, layer2_weights) + layer2_biases\n",
    "    loss = tf.reduce_mean( \\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    tf_l2_feature * (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    hidden_layer_valid_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, layer1_weights) + layer1_biases)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_layer_valid_prediction, layer2_weights) + layer2_biases)\n",
    "    hidden_layer_test_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, layer1_weights) + layer1_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_layer_test_prediction, layer2_weights) + layer2_biases)\n",
    "\n",
    "print('Tensorflow Graph created')\n",
    "\n",
    "num_steps = 101\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: best_l2_constant}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)        \n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using L2 Regularization for Neural Network Model (1 Layer) with Dropout')\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "# Buildig the Network\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_l2_feature = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    layer1_biases = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    layer2_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    layer2_biases = tf.Variable(tf.zeros([num_labels]))    \n",
    "\n",
    "    # Training computation.\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, layer1_weights) + layer1_biases)\n",
    "    hidden_layer_with_dropout = tf.nn.dropout(hidden_layer)\n",
    "    logits = tf.matmul(hidden_layer_with_dropout, layer2_weights) + layer2_biases\n",
    "    loss = tf.reduce_mean( \\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    tf_l2_feature * (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    hidden_layer_valid_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, layer1_weights) + layer1_biases)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_layer_valid_prediction, layer2_weights) + layer2_biases)\n",
    "    hidden_layer_test_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, layer1_weights) + layer1_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_layer_test_prediction, layer2_weights) + layer2_biases)\n",
    "\n",
    "print('Tensorflow Graph created')\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
