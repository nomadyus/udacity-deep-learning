{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L2 Regularization for Logistic Model\n",
      "Tensorflow Graph created\n",
      "Initialized\n",
      "Minibatch loss at step 0: 19.363379\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 10.9%\n",
      "Minibatch loss at step 500: 2.585188\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 1000: 1.677919\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1500: 1.015040\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2000: 0.788029\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 2500: 0.863024\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 3000: 0.756256\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.9%\n"
     ]
    }
   ],
   "source": [
    "print('Using L2 Regularization for Logistic Model')\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_l2_feature = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + tf_l2_feature * tf.nn.l2_loss(weights)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "print('Tensorflow Graph created')\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunning the L2 Regularization constant\n",
      "L2 Regularization constant of 0.000100 performed with 89.76% accuracy\n",
      "L2 Regularization constant of 0.000126 performed with 89.99% accuracy\n",
      "L2 Regularization constant of 0.000158 performed with 89.92% accuracy\n",
      "L2 Regularization constant of 0.000200 performed with 90.55% accuracy\n",
      "L2 Regularization constant of 0.000251 performed with 90.71% accuracy\n",
      "L2 Regularization constant of 0.000316 performed with 90.61% accuracy\n",
      "L2 Regularization constant of 0.000398 performed with 90.76% accuracy\n",
      "L2 Regularization constant of 0.000501 performed with 91.42% accuracy\n",
      "L2 Regularization constant of 0.000631 performed with 92.20% accuracy\n",
      "L2 Regularization constant of 0.000794 performed with 92.55% accuracy\n",
      "L2 Regularization constant of 0.001000 performed with 92.99% accuracy\n",
      "L2 Regularization constant of 0.001259 performed with 93.13% accuracy\n",
      "L2 Regularization constant of 0.001585 performed with 93.15% accuracy\n",
      "L2 Regularization constant of 0.001995 performed with 92.91% accuracy\n",
      "L2 Regularization constant of 0.002512 performed with 92.63% accuracy\n",
      "L2 Regularization constant of 0.003162 performed with 92.44% accuracy\n",
      "L2 Regularization constant of 0.003981 performed with 92.12% accuracy\n",
      "L2 Regularization constant of 0.005012 performed with 91.77% accuracy\n",
      "L2 Regularization constant of 0.006310 performed with 91.14% accuracy\n",
      "L2 Regularization constant of 0.007943 performed with 90.90% accuracy\n"
     ]
    }
   ],
   "source": [
    "print('Tunning the L2 Regularization constant')\n",
    "\n",
    "num_steps = 3001\n",
    "l2_constant_values = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_values = []\n",
    "max_accuracy, best_l2_constant = 0, 0\n",
    "\n",
    "for l2_constant in l2_constant_values:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: l2_constant}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "        if test_accuracy == max(max_accuracy, test_accuracy):\n",
    "            max_accuracy = test_accuracy\n",
    "            best_l2_constant = l2_constant\n",
    "        accuracy_values.append(test_accuracy)\n",
    "        print('L2 Regularization constant of %f performed with %.2f%% accuracy' % (l2_constant, accuracy_values[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot the L2 Regularization loss for our Test\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEMCAYAAADDMN02AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmDBjid8YY8KMJX5jjAkz/z/Pt4ZS4RnRSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14cbf3f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum accuracy is 93.15% with L2 Regularization constant of 0.001585\n"
     ]
    }
   ],
   "source": [
    "print('Plot the L2 Regularization loss for our Test')\n",
    "plt.semilogx(l2_constant_values, accuracy_values)\n",
    "plt.grid(True)\n",
    "plt.title('Accuracy against L2 regularization (Logistic Regression)')\n",
    "plt.show()\n",
    "print('Maximum accuracy is %.2f%% with L2 Regularization constant of %f' % (max_accuracy, best_l2_constant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using L2 Regularization for Neural Network Model (1 Layer)\n",
      "Tensorflow Graph created\n",
      "Initialized\n",
      "Minibatch loss at step 0: 600.770020\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 24.4%\n",
      "Minibatch loss at step 500: 197.260208\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1000: 115.536995\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1500: 68.778358\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 2000: 41.314751\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2500: 25.213100\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3000: 15.520422\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.7%\n",
      "Test accuracy: 93.2%\n"
     ]
    }
   ],
   "source": [
    "print('Using L2 Regularization for Neural Network Model (1 Layer)')\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "# Buildig the Network\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_l2_feature = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    layer1_biases = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    layer2_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    layer2_biases = tf.Variable(tf.zeros([num_labels]))    \n",
    "\n",
    "    # Training computation.\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, layer1_weights) + layer1_biases)\n",
    "    logits = tf.matmul(hidden_layer, layer2_weights) + layer2_biases\n",
    "    loss = tf.reduce_mean( \\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    tf_l2_feature * (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    hidden_layer_valid_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, layer1_weights) + layer1_biases)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_layer_valid_prediction, layer2_weights) + layer2_biases)\n",
    "    hidden_layer_test_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, layer1_weights) + layer1_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_layer_test_prediction, layer2_weights) + layer2_biases)\n",
    "\n",
    "print('Tensorflow Graph created')\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunning the L2 Regularization constant\n",
      "Accuracy of 89.27% for L2 parameter constant of 0.000100\n",
      "Accuracy of 88.92% for L2 parameter constant of 0.000126\n",
      "Accuracy of 89.82% for L2 parameter constant of 0.000158\n",
      "Accuracy of 90.42% for L2 parameter constant of 0.000200\n",
      "Accuracy of 90.31% for L2 parameter constant of 0.000251\n",
      "Accuracy of 91.24% for L2 parameter constant of 0.000316\n",
      "Accuracy of 91.17% for L2 parameter constant of 0.000398\n",
      "Accuracy of 91.26% for L2 parameter constant of 0.000501\n",
      "Accuracy of 92.31% for L2 parameter constant of 0.000631\n",
      "Accuracy of 92.54% for L2 parameter constant of 0.000794\n",
      "Accuracy of 92.85% for L2 parameter constant of 0.001000\n",
      "Accuracy of 93.39% for L2 parameter constant of 0.001259\n",
      "Accuracy of 93.27% for L2 parameter constant of 0.001585\n",
      "Accuracy of 93.19% for L2 parameter constant of 0.001995\n",
      "Accuracy of 92.71% for L2 parameter constant of 0.002512\n",
      "Accuracy of 92.42% for L2 parameter constant of 0.003162\n",
      "Accuracy of 92.28% for L2 parameter constant of 0.003981\n",
      "Accuracy of 91.71% for L2 parameter constant of 0.005012\n",
      "Accuracy of 91.28% for L2 parameter constant of 0.006310\n",
      "Accuracy of 90.85% for L2 parameter constant of 0.007943\n"
     ]
    }
   ],
   "source": [
    "print('Tunning the L2 Regularization constant')\n",
    "\n",
    "num_steps = 3001\n",
    "l2_constant_values = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_values = []\n",
    "max_accuracy, best_l2_constant = 0, 0\n",
    "\n",
    "for l2_constant in l2_constant_values:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: l2_constant}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        test_accuracy = accuracy(test_prediction.eval(), test_labels)\n",
    "        if test_accuracy == max(accuracy_values, test_accuracy):\n",
    "            max_accuracy = test_accuracy\n",
    "            best_ls_constact = l2_constant\n",
    "        accuracy_values.append(test_accuracy)\n",
    "    print('Accuracy of %.2f%% for L2 parameter constant of %f' % (accuracy_values[-1], l2_constant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot the L2 Regularization loss for our Test\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEMCAYAAADNtWEcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd8FHX6wPHPk14ILYFQE3qTJoQu\nxbMrShEVEcGK9X62O9t5dr1Tzzs9sRynd9IUCxawo16w0IP0DpIQegklvX1/f8ygS0zZJLuZLc/7\n9coLdnbKs7PfnWe+ZWbEGINSSqngE+J0AEoppZyhCUAppYKUJgCllApSmgCUUipIaQJQSqkgpQlA\nKaWClCYAh4jIVSLyldNx+CoReVNEnqzF8p+LyGRPxmSvd72IjPD0eu113yQiL3hj3coiIqkicoPT\ncdSWiFwjIj9U8N4lIjLHnfXUKgHYOzNLRCJrs55gZIyZbYw5t7brEREjIh0qeb+ygvI3EdkqIidE\nZJOITKptPL7CGHOBMWZ6bdZRXhIyxpxmjEmtVXDlbysCeAh4zmXaNBHZLCKlInJNdWN1moi0scvn\np2WmzxKRRx0Kq0Ii8qgd72Uu08LsaW3cWH6EiGR6M0Z3GGPmAd1FpGdV89Y4Adg7ZChggEtqup4a\nbjusLrcXwHKAi4EGwGTgRREZ7M6CvvodiMUfa7ajgE3GmN0u01YDtwIrnQnJfVWUh4EiMsThGNx1\nBHhcREI9sC6vcPNzvg1MqWqm2vxQJgFLgDexDh6/EJFoEXleRNJF5JiI/CAi0fZ7Z4jIIhE5KiK7\nTp7ZlK2alT1ztbPwbSKyFdhqT3vRXsdxEUkTkaEu84eKyIMist0+w00TkdYi8rKIPF8m3vkicmd5\nH7KKbUSLyHS7FrRRRO51PQMQkftdtr9BRMZU8fluts/Is+w4xX6vg4gstPflIRF5x57+nb34ahHJ\nFpErKv3GyjDGPGKM2WSMKTXGLAW+BwZVsB9GiEimiNwnIvuA/9rTR4rIKvv7XOR61iEifUTkJ/vz\nvyci75w8Sy2vZiIV1GZEpJGIfCIiB+1984mItHJ5P1VEnhKRH4FcoJ1reRKRk/vn5J8RuxnHjmuf\nvW+/E5HT7OlTgKuAe+1l5tvTd4rI2fb/I0XkBRHZY/+9IHZt2GV/3SMiB0Rkr4hcW8nXcQGwsMz3\n87Ix5hsgv5LlqlRRGRaRZiKSKyLxLvP2tfdzuP36OrtsZ4nIlyKS7DLvb36TFXgWqLB2UkUZOqVM\niEtNp7wyWVVZccMXQCEwsYJYI8WqOWeIyH4ReU2s40As8DnQwqWctRCRPBFJsJd9SESKRaS+/fpJ\nsZv8RKSBiMyw40635w2x37tGRH4UkX+IyBHg0XLiek6s42wDe1IqcFFVH7a2CWC2/XeeiCS6vPc3\noC8wGGgM3AuUikgS1k56CWgC9AZWVWObo4EBQDf79XJ7HY2Bt4D3RCTKfu9u4ErgQqA+cB3WwWE6\ncKXLzk0AzsLKmOWpbBuPAG2AdsA5/LbQbMeqJTUAHgNmiUjzSj7fSKAf0Au4HDjPnv4E8BXQCGiF\ntf8wxgyz3+9ljKlnjHmnknVXSqwE3Q9YX8lszbD2QzIwRUT6AP8BbgLigX8B8+wfSQTwIdYJQmOs\n/TumvJW6IQQr4SQDSUAeMLXMPFdjnfHEAemubxhjTu6feljlYjO/nlV/DnQEmtrTZtvLTLP//6y9\n7MXlxPUnYCBW+egF9MdqxjmpGdZ33xK4HnhZRBpV8Bl72HF5Q7ll2BizD+tAcbnLvBOBOcaYIhEZ\nDTwIjMX6vX7Pb38nZX+T5XkZ6HQycbqqrAy5+dlOKZO4V1YqY4A/A4+cTIJlPAN0wtqfHbC+24eN\nMTlYSXzPybJmjNmDte+H28sOwyqbQ1xen0z6L2GVlXb2/JMA1xOGAcAOrHL61MmJIhIiIv8GegLn\nGmOO2W9tBNqcTDYVf1pjqv0HnAEUAQn2603AXfb/Q7B2eq9ylnsA+LCCdaYCN7i8vgb4weW1AX5X\nRVxZJ7eL9WMaVcF8G4Fz7P/fDnxWjc/uuo0dwHku790AZFay7KqTMVXw+c5wef0ucL/9/xnANKBV\nOes0QIdKtnnKdiqZbzrW2Y9U8P4IrDOjKJdprwJPlJlvM1YBHgbsdl0f8APwZEVxuX4WrMTxZAWx\n9AayypSdxysrTy7l9gDQqYL1NrRjaFBRDMBO4Gz7/9uBC13eOw/Y6bK/8oAwl/cPAAMr2PZW4PwK\n3vsBuKaK76/C/VVFGb4C+NH+fyiwD+hvv/4cuN5luRCsk6hkl++rwt8k1smRAcKwmrKW2NNnAY9W\nVYbKK9+un7O8MulmWbmhgnkfBWbZ/18K3GLHbuzPIljNpu1dlhkE/OwST2aZdT4B/NNezz7gDuCv\nQJRdPhLs/V4AdHNZ7iYg1eW3klFmvdfYMb4DzAUiyrwfbsedVFlZqGkNYDLwlTHmkP36LX5tBkqw\nP9z2cpZrXcF0d+1yfWFXrzfa1fejWBk0wY1tTefXs/WJwMyKNljFNlqUialsfJNcqrZHge4uy5Zn\nn8v/c4F69v/vxSp8y8QahXJdJeuoNhF5zo7tcmOXngocNMa4NkckA/ec/Hz2Z2yNtV9aALvLrO+U\n/VON+GJE5F921fg48B3QUE5tp6103SLSGiupTjbGbLGnhYrIX8VqpjuOdXCHyr8jVy04tbaRbk87\n6bAxptjltet3WlYWVu3F46oowx8D3UTkZC32mDFmmf1eMla/0Mnv9ghWOWzpsnp3v9N/A4kiUrYm\nVVkZcscpZdLNsuKOh7BqeFEu05oAMUCaS6xf2NMrshArMfQB1gILsE6QBgLb7GNoAhDBb8tSVfu5\nA1bf0WPGmMIy750sS0cria36CcBuKrgcGC5W2+k+4C6gl4j0Ag5htVm2L2fxXRVMByuzxri8blbO\nPL8cTMRqx7zPjqWRMaYhcAyrgFa1rVnAKDversBH5c3kxjb2YjXJnNTaZdlkrEJ/OxBvL7vOZVm3\nGWP2GWNuNMa0wDozeEUqGflTHSLyGFbV9VxjzPGqQinzehfwlDGmoctfjDHmbax901JEXD9va5f/\nn/J9i0h53/dJ9wCdgQHGmPpYtQs4dV9WmLjsMvsR8IIx5nOXtyZg/YDOxjootimz3qpulbsH6wB2\nUpI9rSbWYDUteFRVZdg+eL6L1d9xNaeeDO0Cbirz/UYbYxa5zOPW7YSNMUVYzaBPcOr3VlkZAitp\nVnZcKLt9d8qKO/EuALZh1VxOOoR11n6aS6wNjNW0WF4sAIvseMYAC40xG7DKyUX82vxzCKtFpWxZ\nch0QUN66N2I1E30uIp3LvNcVqzZa6W+6JjWA0UAJVptfb/uvK1b74CRjTClWm97f7U6QUBEZZLfp\nzQbOFpHLxRpeFS8ive31rgLG2hm8A1abaWXigGLgIBAmIg9jtfWf9DrwhIh0FEtPsTu7jDGZWG1z\nM4G5xpi8Gm7jXeABsTqeWmId7E+KxfrSDgKI1QHYvYrPVC4RuUx+7cjKstdbYr/ej9VuWMUqJMr1\nz574ANZB8BxjzOEahPZv4GYRGWDv41gRuUhE4oDFdoy329/1KKw28pNWA6eJSG87nkcr2U4c1g/v\nqIg0xup7qY7/YI2webac9RYAh7EOMk+Xeb+qffs28JCINLH7kh7GOrmoic/4ta0YsIaG2vtGgHD7\nu6vsNxta5nuOoOoyDFYT4zVYo/lc438Nq3yf7BhvIC5DJGtgJhAJnO8yrbIyBNZxYYJ9HDmfMvuo\nHLUtK67+hFX7BsA+tv0b+IeINAUQkZYicrKvbj8QL792xGKMyQXSgNv49YC/COtEbqE9TwnWseQp\nEYmzTx7vxo2yZCfKB4GvRcT1hHc4VhNepWqSACYD/zXGZNhnpvuM1Zk0FbhKrCFKf8Cq7izHqjY+\nA4QYYzKwOmXvsaevwuo8A/gHVnvefqwmmtlVxPEl1gfcglVdyufUatLfsXbqV8Bx4A0g2uX96Vgd\nbxU2/7ixjceBTOBn4GvgfawDCnamfx7rQLjf3taPVXymivQDlopINjAPuMMY87P93qPAdLtKenkF\nyw/G+lH88md/T09jnWlslV9HLjzoblDGmBXAjVjffRbWGdM19nuFWJ2H12NVQycCn/Dr/tmCtf++\nxmr/LvdaBdsLWN/dIayRZ1+4G6NtPDBGTh0JNBTrwJeOdaa1wV63qzewmkeOikh5tcQngRVYZ+9r\nsTqRazoWfz7QRURcmz6+wvq+BmP1AeXx6xltee7n1O/5W6ouwxhjfgRKgZXGmJ0u0z/E+u3OsZtT\n1mHVFmvEPtA9gtVpe3JahWXIdgfWUOWjWLWUcmvrLmpbVlzj/RFYVmbyfXaMS+x98jXWGT7GmE1Y\nJwU77DJz8rtciNUmv8zldRxW89RJv8eqFe/A+i28hXXi4k6c07F+S9/Kr9crXInVoV4pqbzJN3CJ\nyDCsDNvGzuyeWOctwHhjTFVnKUFJRJYCrxlj/ut0LL5IrKGn3Ywx5Q5J9vK2vwXeMsa8XtfbVp4l\nVj/L1caYik4If503GBOAWMO75gCrjTGP12I9zbGaCBZjDSX8FJhqjNHL+QERGY41ouMQ1tnba0A7\nY8xeRwNTpxCRflidk62NMSecjkfVHZ+8mtObRKQrVrV9NaeOs62JCKxqVlusKuoc4JVarjOQdMZq\nhquHNSJrnB78fYuITMfq17tDD/7BJyhrAEoppfRuoEopFbQ0ASilVJCq0z6AhIQE06ZNmxotm5OT\nQ2xsrGcDUspNWv6Uk9LS0g4ZYyq74rhG6jQBtGnThhUrVtRo2dTUVEaMGOHZgJRyk5Y/5SQRSa96\nrurTJiCllApSmgCUUipIaQJQSqkgpQlAKaWClCYApZQKUpoAlFIqSGkCUMrLCopLWL7zCEdyyj60\nSSlnBd3N4JSqS/uO5XPTrDRW77KezNcmPoY+yY3ok2T9dW4WR2hItR8Sp5RHaAJQykvS0rO4eVYa\nOQXFPDm6O9kFxaxMz+K7LQf5YKX1tL/YiFB6JzW0EkJyI/q0bkSDmHCHI1fBQhOAUl7w7opdPPTh\nOpo1iGLW9QPo3OzX570bY9h1JI+VGVmkpWexMiOLV1K3U1Jq3Zm3fZNY+p6sJSQ3okOTeoRoLUF5\ngSYApTyoqKSUpz7dyJuLdnJGhwSmTjidhjERp8wjIiTFx5AUH8Po01sCkFNQzOrMo/yUcZSV6Vks\n2LCfd1dkAhAXFcaAtvE8PLIbSfExv9mmUjWlCUApDzmSU8hts1eyeMdhrj+jLQ9c0IWwUPfGWcRG\nhjG4fQKD2ycAVi3h50M5rMw4ysqMLD5ZvYeLp/7AC+N7c2bnpt78GCqI6CggpTxg497jXDL1B9Iy\nsnj+sl78eWQ3tw/+5RER2jWpx7i+rXh6TA8++f1QWjaM5ro3l/PC11soLdUHOana0wSgVC19tnYv\nY19ZRFFJKe/eNIhL+7by+DaS4mOYe8tgxpzekhe+3sr105dzLLfI49tRwUUTgFI1VFpqeP6rzdw6\neyVdmscx//Yz6N26ode2Fx0RyvOX9eKJ0d35YdshLp76Axv2HPfa9lTg0wSgVA2cyC9iyswVvPTt\nNi5PacWcKQNpWj/K69sVEa4emMycKYMoKC5h7Ks/8uFPmV7frgpMmgCUqqafD+Uw5pVF/G/zQR67\n5DSeubQnkWGhdRpD3+RGfPL7ofRq1ZC73lnNwx+vo7C4tE5jUP5PE4BS1bBwy0FGTf2Bw9kFzLy+\nP5MHt0HEmTH6TeIimX3DAG4c2pYZi9MZP20x+47lOxKL8k+aAJRygzGGad9t59r/LqNFw2jm3X7G\nL0M2nRQWGsKfLurGyxP6sGnfCUa+9D1Ldhx2OizlJzQBKFWF/KISpq0p4OnPNnF+92Z8cOtgWjf2\nrQuyLurZnI9vG0L9qHCuen0pr3+/A2N0qKiqnFsJQETuEJF1IrJeRO60pz0hImtEZJWIfCUiLbwb\nqlLO+Ovnm1i8t4Q/nNuJlyf0ISbCN6+f7JgYx8e3D+Hsrk158tON3P72T+QUFDsdlvJhVSYAEekO\n3Aj0B3oBI0WkI/CcMaanMaY38AnwsFcjVcoBKzOymL54J2clhXH77zo61t7vrriocF6b2Jf7zu/C\n52v3MvrlH9l+MNvpsJSPcqcG0BVYYozJNcYUAwuBMcYY1wHIsYDWN1VAKSwu5f65a2hWP4pxnSKq\nXsBHiAi3jGjPzOsHcDinkFFTf+SLdfucDkv5IKmqnVBEugIfA4OAPOAbYIUx5vci8hQwCTgGnGmM\nOVjO8lOAKQCJiYl958yZU6NAs7OzqVevXo2WVaom5m0v5IOtRdzZJ5IOMfl+Wf4O55UydVUBPx8r\nZXSHcEa1D/f5Woz6rTPPPDPNGJPi6fVWmQAAROR64DYgG9gA5Blj7nJ5/wEgyhjzSGXrSUlJMStW\nrKhRoKmpqYwYMaJGyypVXdsOZHPhi99zzmmJvDyhj1+Xv/yiEh78cC0frNzNJb1a8Oy4nkSF1+11\nC6p2RMQrCcCtTmBjzBvGmD7GmGHAEWBrmVneAi71dHBKOaG01PDgB2uJjgjl0YtPczqcWosKt24h\n8cfzOjNv9R7GT1vCgRN6vYByfxRQU/vfJGAs8LbdEXzSJcAmz4enVN2bs3wXy3Ye4U8XdqVJXKTT\n4XiEiHDbmR14bWIfNu87weipP+p9hJTb1wHMFZENwHzgNmNMFvBXe2joGuBc4A5vBalUXdl/PJ+/\nfLaRQe3iuSzF83f1dNr53Zvz3s2DKDUw7rVFLNiw3+mQlIPcbQIaaozpZozpZYz5xp52qTGmuz0U\n9GJjzG7vhqqU9z3y8XoKS0p5emyPgO0s7d6yAR/fPoQOTesxZeYK/rVwu140FqT0SmClbF+s28cX\n6/dxx9kdaZsQ63Q4XpVYP4p3pgziwu7N+cvnm7j3/TV6M7kg5JuXNCpVx47nF/Hwx+vo2rw+Nw5t\n53Q4dSI6IpSXrjyd9k1i+ee320g/kstrE/vSONZ/rnlQtaM1AKWAZz7fxKHsAv46tgfhtXiUo78J\nCRHuPrczL47vzapdRxn98o9sO3DC6bBUHQmekq5UBZbvPMLspRlcO6Qtvbz4RC9fNqp3S96+cSC5\nhcWMeWUR3235zTWdKgBpAlBBraC4hPvnrqFlw2juPqeT0+E4qm9yIz66bQgtG0Zz7ZvLmb5op9Mh\nKS/TBKCC2sv/2872gzk8NaY7sZHaJdaqUQzv3zKYEZ2a8Mi89Tz88TqKS7RzOFBpAlBBa8v+E7ya\nuo3RvVswonNTp8PxGfUiw5g2KYUpw9oxY3E61765nGN5RU6HpbxAE4AKSqWlhvvnrqFeZBh/HtnN\n6XB8TmiI8OCFXXnm0h4s3n6Ysa/8yMETBU6HpTxME4AKSrOWprMy4yh/HtmN+HqBcbsHb7iiXxIz\nrx/A7qN53DIrTa8VCDCaAFTQ2XM0j2c+38TQjgmMOb2l0+H4vEHt43luXC9WpGfxyLx1etVwANFe\nLxVUjDE8/PE6Sg08PSZwb/fgaRf3asHGvcd5JXU73Vo04OqByU6HpDxAawAqqHy2dh9fbzzA3ed0\n8rkHu/u6e87tzO+6NOWxeetZsuOw0+EoD9AEoILGsdwiHpm3nh4tG3DtkDZOh+N3QkOEF8b3Jik+\nhltnryQzK9fpkFQtaQJQQePpzzaSlVvIX8b2ICyIbvfgSfWjwvn3pBSKSkqZMiONvMISp0NStaC/\nAhUUFm0/xDsrdnHD0LZ0b9nA6XD8Wvsm9fjnlaezcd9x/vj+au0U9mOaAFTAyy8q4cEP1pLUOIY7\nzwru2z14ypmdm3Lf+V34ZM1eXl243elwVA1pAlAB7WhuITdMX8HOw7k8PaYH0RH6MHRPuWlYOy7p\n1YLnvtzMt5v0yWL+SBOAClib953gkqk/suznIzw7ridndExwOqSAIiI8c2lPTmtRnzveXsW2A9lO\nh6SqSROACkhfrNvHmFd+JL+ohDk3DeTylNZOhxSQoiNC+dfVKUSGhzBlxgq9Z5Cf0QSgAkppqeGF\nr7dw86w0OibGMf/3Z9AnqZHTYQW0lg2jeXViX3Zl5XLHnJ8oKdVOYX+hCUAFjOyCYm6elcYLX2/l\n0j6teGfKQBLrRzkdVlDo16Yxj13SndTNB3nuy81Oh6PcpLeCUAEh/XAON85YwfaDOTw8shvXDmmj\nt3moYxMGJLF+zzFeW7idrs3jGNVb77Pk6zQBKL/3w9ZD3PbWSgCmX9tfO3sd9MjFp7F1fzb3vr+G\ndgn16NFKr7nwZdoEpPyWMYbXv9/BpP8spVn9KObdPkQP/g6LCAvhlYl9iI+N4KaZKziUrc8Q8GWa\nAJRfyi8q4Z73VvPkpxs5p1siH9w6mOT4WKfDUkBCvUimTUrhSG4ht85aqc8Q8GGaAJTf2Xcsnyv+\ntZgPVu7mrrM78epVffV5vj6me8sGPDuuF8t2HuGx+eudDkdVQH81yq+kpWdx86w0cguK+dfVfTnv\ntGZOh6QqcIn9DIFXU7fTtXl9JuozBHyOJgDlN95dvouHPlpHswZRzLp+AJ2bxTkdkqrCH87tzKa9\nx3l03nqO5BRy0/B2RIbp7Th8hTYBKZ9XVFLKo/PWc+/cNQxo15h5tw/Rg7+fCA0RXrzydM7v3oy/\nL9jCBS98z6Jth5wOS9k0ASif9+/vd/Dmop3ccEZb/ntNPxrGRDgdkqqG+lHhTJ3QhxnX9afEGCa8\nvpQ75/zEwRM6QshpmgCUTysuKWXm4nSGdIjnoZHd9EEufmxYpyZ8eecw/u+sjny2dh+/ez6VmUvS\n9dYRDtJfk/JpX288wN5j+Uwa1MbpUJQHRIWHcvc5nfjizqH0bNWAP3+0jrGvLmLd7mNOhxaUNAEo\nnzZj8U5aNozmrC5NnQ5FeVC7JvWYdf0AXhzfm91ZeVwy9Qcem7+eE/l6N9G6pAlA+ayt+0+waPth\nJgxI0qafACQijOrdkm/uGc5VA5J5c9FOzv77Qj5ds1cfM1lH9FelfNbMJelEhIYwvp/eyz+QNYgO\n54nR3fno1iE0iYvktrdWcs1/l5N+OMfp0AKeJgDlk07kFzE3LZORvZoTXy/S6XBUHejVuiEf33YG\nj17cjbT0LM79x3e89M1WCopLnA4tYLmVAETkDhFZJyLrReROe9pzIrJJRNaIyIci0tC7oapg8sHK\n3eQUlmjnb5AJDRGuGdKWb+4ZztndEnl+wRYueFGvHfCWKhOAiHQHbgT6A72AkSLSEVgAdDfG9AS2\nAA94M1AVPIwxzFi8k16tGtC7tZ5XBKPE+lG8PKEP06/rT0mpde3Ahz9lOh1WwHGnBtAVWGKMyTXG\nFAMLgTHGmK/s1wBLgFbeClIFl0XbD7P9YA5X69l/0BtuXzswsF1j7pu7lpUZWU6HFFDcuRfQOuAp\nEYkH8oALgRVl5rkOeKe8hUVkCjAFIDExkdTU1BoFmp2dXeNllX/558p86oVD/aNbSU3d5nQ4gJY/\np01sY9ixz3DN64t5ZFAU8dHafekJVSYAY8xGEXkGq8knG1gNnDzzR0T+ZL+eXcHy04BpACkpKWbE\niBE1CjQ1NZWaLqv8x+6jeaz68ltuGt6ec8/q4nQ4v9Dy57wuvU8w5uVFvLElnPdvGURMhN7Lsrbc\nSqPGmDeMMX2MMcOAI8BWABGZDIwErjI6cFd5wOwl6QBcNSDJ4UiUr+nQNI6XJpzOpn3Hufud1ZTq\nLSRqzd1RQE3tf5OAscDbInI+cB9wiTEm13shqmCRX1TCnOW7OKtrIq0axTgdjvJBIzo35U8XdeOL\n9fv4x9dbnA7H77lbh5pr9wEUAbcZY7JEZCoQCSwQEbA6im/2UpwqCHy2di9HcgqZrJ2/qhLXDWnD\nln0neOnbbXRoWo9RvVs6HZLfcisBGGOGljOtg+fDUcFs+uJ02jWJZUiHeKdDUT5MRHhidHd+PpzD\nH99fQ3J8rA4XriHtSlc+YfWuo6zedZRJA5Oxa5RKVSgiLITXJvYlsX4kU2asYN+xfKdD8kuaAJRP\nmLE4ndiIUC7tq5eTKPc0jo3gjcn9yCko5sYZK8gr1FtGVJcmAOW4IzmFzF+zhzF9WhIXFe50OMqP\ndEqM459Xns66Pcf4w3s6Mqi6NAEox72zfBeFxaV63x9VI2d1TeSBC7rw6dq9vPjNVqfD8St6JYVy\nVEmpYdaSdAa1i6dToj7oXdXMjUPbsXV/Ni9+s5WOifUY2bOF0yH5Ba0BKEd9u+kAu4/mMWlQstOh\nKD8mIjw5pjv92jTiD++tZm2mPmLSHZoAlKNmLN5J8wZRnNMt0elQlJ+LDAvl1Yl9iY+N5IYZy9l/\nXEcGVUUTgHLM9oPZfL/1EBP66yMflWck1Ivk9ckpnMgvZsqMFeQX6cigyuivTjlm5uJ0wkOF8f31\nvj/Kc7o2r8+L409nze5j/PH9Nfp84UpoAlCOyC4oZm5aJhf1aE6TOH3ko/Ksc7olcu95XZi/eg9T\nv/WNW4r7Ih0FpBzx4U+7OVFQrA99UV5z8/B2bN1/gucXbKFD03pc0KO50yH5HE0Aqs4ZY5i5eCfd\nW9anT5Lew0V5h4jw9Nge/Hw4h7vfXU12QTHj+rbSW4240CYgVeeW7DjClv3ZTBrURn+MyquiwkOZ\ndnUKp7Wozx/fX8MV/1rClv0nnA7LZ2gCUHVuxuKdNIwJ55JeerGO8r4mcZG8e9Mgnr20J1sOnODC\nF7/nmS826b2D0ASg6tjeY3l8tWE/V6S0Jio81OlwVJAICREu79eab+8Zwdg+LXk1dTtn/30h32zc\n73RojtIEoOrUW0szKDWGiQP1yl9V9xrHRvDsuF68e9MgYiNDuX76CqbMWMHuo3lOh+YITQCqzhQU\nl/D2sgx+17kprRvrIx+Vc/q3bcwnvx/Kfed34butBznn7wuZ9t12ikpKnQ6tTmkCUHXmi3X7OJRd\nyKTBbZwORSkiwkK4ZUR7Ftw1nMHt43n6s01c/NIPpKUfcTq0OqMJQNWZ6Yt20iY+hqEdEpwORalf\ntG4cw+uT+zHt6r4czyvi0lcXc//cNWTlFDodmtdpAlB1Yt3uY6zMOMrVg9oQEqJDP5XvOfe0Ziy4\nezg3DWvHe2mZnPX3hbyflhmtCXI8AAAU2klEQVTQt5LQBKDqxIzFO4kOD2WcPvJR+bDYyDAeuLAr\nn/7fGbRLiOUP763mimlL2Bqg1w5oAlBel5VTyMer9jD69JY0iNZHPirf16VZfd69aRDPXNqDLftP\ncOE/v2fJjsNOh+VxmgCU172XtouC4lJ96IvyKyEhwhX9kvjm7uE0bxDNAx+sDbjbS2sCUF5VWmqY\nvTSDfm0a0bV5fafDUara4utF8vSYHvx8KIeXvg2sZw5rAlBe9cO2Q6QfztULv5RfO6NjAuP6tuJf\nC3ewce9xp8PxGE0AyqtmLkknPjaC87s3czoUpWrlTxd2pUF0OPfPXUNJaWCMDNIEoLxmz9E8vtm4\nn8v7tSYyTO/7o/xbo9gIHrnkNFZnHmP6op1Oh+MRmgCU18xZloEBJugjH1WAuLhnc87s3IS/fbWZ\nzKxcp8OpNU0AyiuKSkqZs3wXIzo10fv+qIAhIjw5pgcAD320zu8vEtMEoLxiwYb9HDhRoJ2/KuC0\nbBjNH8/rTOrmg8xbvcfpcGpFE4DyillL0mnZMJoRnZs6HYpSHjdpUBt6t27I4/M3+PU9gzQBKI/b\ndiCbRdsPM2FAEqF63x8VgEJDhL9e2oNjeUU8+elGp8OpMU0AyuNmL00nPFS4PKW106Eo5TVdmtXn\n5uHtmbsykx+2HnI6nBrRBKA8Kq+whLlpmZzfvTlN4iKdDkcpr7r9dx1olxDLgx+u9ctnDGsCUB41\nf/UejucXM3GADv1UgS8qPJSnx/Yg40guL3y9xelwqs2tBCAid4jIOhFZLyJ32tMus1+XikiKd8NU\n/mLW0nQ6Jdajf9vGToeiVJ0Y2C6eK/u35vUffmbd7mNOh1MtVSYAEekO3Aj0B3oBI0WkI7AOGAt8\n59UIld9YvesoazKPcdWAZES081cFj/sv6Erj2Ajum7uGYj96rrA7NYCuwBJjTK4xphhYCIwxxmw0\nxmz2bnjKn8xakk50eChj+rR0OhSl6lSD6HAev+Q01u85zn9+/NnpcNzmTgJYBwwTkXgRiQEuBHR4\nhx95+X/buODF7zmcXeC1bRzLLWL+GuuhL/Wj9KEvKvic370Z53RL5O8LtpBx2D9uExFW1QzGmI0i\n8gywAMgGVgPF7m5ARKYAUwASExNJTU2tUaDZ2dk1XjaYfZ1exKyN1oUqN0z7H/93eqRXmme+3FlE\nflEpXcIOBOT3pOVPuePCpqX8sKWUW/6zkD+kRPl8U2iVCQDAGPMG8AaAiDwNZLq7AWPMNGAaQEpK\nihkxYkT1owRSU1Op6bLBat7qPcz+8ifO7ppI3+RGPPPFJg7Ua88V/Tw7QscYw+NpCzk9KYbJlwzx\n6Lp9hZY/5a7shun8+aN1HKnfkUt9/BnY7o4Camr/m4TV8fu2N4NStffdloPc8+4q+iU3ZuqE07lp\nWDsGtYvnsfkbSD+c49FtLd5+mB0Hc5g4QO/7o9RV/ZNISW7EE59u4JAXm109wd3rAOaKyAZgPnCb\nMSZLRMaISCYwCPhURL70WpSqWn7KyOLmWWl0aBrHvyenEBUeSkiI8PzlvQgNEe58Z5VHRyrMWppO\nw5hwLurZ3GPrVMpfhYQIfxnbg9yCEp74ZIPT4VTKrQRgjBlqjOlmjOlljPnGnvahMaaVMSbSGJNo\njDnPu6Eqd2w7cIJr31xOQr1Ipl/XjwbRv3bItmgYzZOju/NTxlFeSd3uke3tP57Pl+v3c1nfVkSF\n60NflALomBjHrWe25+NVe/jf5gNOh1MhvRI4gOw+msfVbywjLCSEmdf3p2lc1G/mGdW7JaN6t+DF\nb7ayatfRWm9zzrJdlJQartLmH6VOccuI9nRoWo+HPlxHToHb42bqlCaAAHEkp5BJbywlO7+Y6df1\nIzk+tsJ5Hx/VncS4SO56ZxW5hTUvmMUlpby9LIOhHRNok1Dx9pQKRpFhoTxzaQ/2HMvj+a988zYR\nmgACQE5BMde+uZzMrDxen5zCaS0aVDp/g+hw/nZ5L3YezuGpWtzK9ptNB9h3PF8f+qJUBfomN2bi\ngGTeXPSzR2rcnqYJwM8VFpdy86w01u0+xtQJfRjQLt6t5Qa3T+DGoe2YvTSDbzftr9G2Zy1Jp3mD\nKM7qog99Uaoi957fmQt7ND+lP85XaALwYyWlhrvfXcX3Ww/xl7E9OKdbYrWWv+fcTnRpFse976+p\n9nC1nYdy+H7rIcb3SyIsVIuRUhWJiwpn6oQ+tPXBZlL95fopYwyPzlvPJ2v28sAFXWr08JXIsFBe\nGN+b43nF3D93bbUecD17aTqhIcL4/npXEKX8lSYAP/XiN1uZuSSdm4a146bh7Wu8ni7N6nPv+Z35\neuN+3lm+y61l8otKeC8tk3O7JZJY/7cjjZRS/kETgB+auXgnL3y9lXF9W3H/BV1qvb7rhrRlSId4\nHv9kAzsPVX2V8Kdr9nI0t4irtfNXKb+mCcDPzF+9h4fnrefsron8dWwPj9xsKiRE+NtlvQhz8yrh\nWUvTadcklkHt3etwVkr5Jk0AfuS7LQe52+X+Pp7sfG3eIJqnxvRg1a6jTP3ftgrnW7/nGD9lHNWH\nvigVADQB+Iny7u/jaRf3asHo3i146dtt/JSRVe48s5ZkEBUewrg+vn2XQ6VU1TQB+IEdB7O5roL7\n+3jaY6O606x+FHe9s+o3l68fzy/io592c3HPFjSI8b0xzUqp6tEE4Aee/WIzJaWmwvv7eFKD6HCe\nv7wX6UdyebLMVcIfrtxNXlGJXvmrVIDQBODjdh/N46sN+5gwILnS+/t40sB28UwZ2o63l2Xw9Qbr\nKmFjDLOWpNOzVQN6tW5YJ3EopbxLE4CPm7UkHYCJAz37FK+q3H1uJ7o2r899c9dw8EQBy34+wtYD\n2frQF6UCiCYAH5ZfVMKcZRmc0y2RVo1i6nTbkWGhvDi+NycKirl/7hpmLc2gflQYF/dqUadxKKW8\nx61nAitnzFu9h6zcIiYPbuPI9jslxnH/+V143H6q0bVD2hAdoQ99USpQaA3ARxljmL5oJ50T4xjk\n5h0+veGawW04o0MCgD70RakAozUAH7UyI4v1e47z1Jjujl5wFRIivHZ1X7YdyKZD03qOxaGU8jyt\nAfioNxelUz8qjDGnt3Q6FOpFhtFbR/4oFXA0Afig/cfz+XztXi5PaU1MhFbSlFLeoQnAB81emkGJ\nMVw9SNvclVLeownAxxQWl/LW0gzO7Ny0zi78UkoFJ00APuaztXs5lF3g2NBPpVTw0ATgY95ctJN2\nCbEMtYdeKqWUt2gC8CGrdx1l1a6jTBqUTEiI3mtfKeVdmgB8yPRFO4mNCOXSvnqvfaWU92kC8BGH\nsgv4ZM1exvVtRVyU3mtfKeV9mgB8xNtLMygsKWWSdv4qpeqIJgAfUFRSyqyl6QztmED7Jnq7BaVU\n3dAE4AO+XL+P/ccLuEbP/pVSdUgTgA+YsSidpMYxjOjc1OlQlFJBRBOAwzbsOc6ynUeYNCiZUB36\nqZSqQ5oAHDZ90U6iw0O5rG9rp0NRSgUZTQAOysop5KNVuxl9eksaxOjQT6VU3dIE4KB3VuyioLiU\nyYP1rp9KqbrnVgIQkTtEZJ2IrBeRO+1pjUVkgYhstf9t5N1QA0tJqWHm4nQGtmtMl2b1nQ5HKRWE\nqkwAItIduBHoD/QCRopIR+B+4BtjTEfgG/u1ctPXG/ez+2ieDv1USjnGnRpAV2CJMSbXGFMMLATG\nAKOA6fY804HR3gkxME1ftJMWDaI4u2ui06EopYKUOwlgHTBMROJFJAa4EGgNJBpj9gLY/+ogdjdt\n2X+CRdsPM3FQMmGh2g2jlHJGlQ+cNcZsFJFngAVANrAaKHZ3AyIyBZgCkJiYSGpqao0Czc7OrvGy\nvmb6+gLCQqB14S5SUzOdDke5IZDKn1IniTGmeguIPA1kAncAI4wxe0WkOZBqjOlc2bIpKSlmxYoV\nNQo0NTWVESNG1GhZX3Isr4iBT3/DyJ7Nee6yXk6Ho9wUKOVP+ScRSTPGpHh6ve6OAmpq/5sEjAXe\nBuYBk+1ZJgMfezq4QPTeil3kFZXoIx+VUo6rsgnINldE4oEi4DZjTJaI/BV4V0SuBzKAy7wVZKAo\nLTXMXJJOSnIjurds4HQ4Sqkg51YCMMYMLWfaYeAsj0fkQ4wxHMouJP1wDkUlhtOTGhIVHlrj9aVu\nOUD64VzuObfSljKllKoT7tYAAlZpqWH/iXx2Hsol/XAOOw//+m/G4RxyCkt+mTcqPISB7eIZ3qkJ\nwzs1oW1CLCLu38Bt+qJ0msZFckH3Zt74KEopVS1BkQCMMWRm5bHz5AH+0K8H+owjuRQUl/4yb3io\n0LpxDG3iYxnQtjFt4mNIToiltNTw/dZDLNxykMfmbwCgVaNohndqwrBOTRjcPr7SRznuOJjNwi0H\nuevsToTr0E+llA8IigTw0rfb+PuCLb+8jgoPIblxLG0TYjmzS1OS7AN+cnwMLRpGV3hb5rPsi7Yy\nDueycOtBFm4+yEc/7Wb20gzCQoS+yY0YZtcOujWvT4jLemYsTic8VLhygN71UynlGwI+AeQXlfDG\nDz9zRocEbv9dB9rEx9I0LvKUg3N1JcXHcHV8MlcPTKawuJS09CwWbjnId1sO8tyXm3nuy80k1Itk\nWMcEhnduQp+kRryflslFPZrTNC7Kg59OKaVqLuATwKdr9nIsr4hbz2zPwHbxHl9/RFgIg9rHM6h9\nPPdf0IUDx/P5bushvttykP9tPsAHP+3+ZV4d+qmU8iUBnwBmL02nXUIsg7xw8C9P0/pRjOvbinF9\nW1FSali7+xjfbTmIMXB6kt4wVSnlOwI6AWzYc5yVGUd56KKu1Rqt4ymhIULv1g3p3bphnW9bKaWq\nEtDDUd5alk5EWAjj+rZyOhSllPI5AZsAsguK+XDlbkb2bE7DmAinw1FKKZ8TsAlg3qo95BSWcNUA\nfdyiUkqVJyATgDGG2UvT6dIsjj5J2v6ulFLlCcgEsDrzGOv3HOeqgcmOdP4qpZQ/CMgEMHtJOjER\noYzu3cLpUJRSymcFXAI4llvE/DV7GNW7ZaX35lFKqWAXcAngg58yyS8q5aoBSU6HopRSPi2gEoAx\nhreWZtCrdUN94IpSSlUhoBLA8p1ZbD2QrWf/SinlhoBKALOXphMXFcbFPbXzVymlqhIwCeBwdgGf\nr93HpX1aER1R88c2KqVUsAiYBPB+WiaFJdr5q5RS7gqIBFBaanhrWQb92zamY2Kc0+EopZRfCIgE\n8OP2Q6QfztWzf6WUqoaASACzl2TQODaC87s3czoUpZTyG36fAPYfz2fBxv1cltKKyDDt/FVKKXf5\nfQJ4Z/kuSkoNE/pr849SSlWHXyeA4pJS3l6WwdCOCSTHxzodjlJK+RW/TgCpmw+y91i+dv4qpVQN\n+HUCmL00naZxkZzVNdHpUJRSyu/4bQLYdSSX1C0HGd+vNeGhfvsxlFLKMX575JyzPAMBrtDOX6WU\nqhG/TACFxaW8szyT33VpSsuG0U6Ho5RSfskvE8CCDfs5lF3AVQOSnQ5FKaX8ll8mgNlL02nZMJph\nnZo4HYpSSvktv0sA2w9ms2j7YSYMSCI0RJwORyml/JbfJYC3l2YQFiJcltLK6VCUUsqv+VUCyC8q\n4f2VmZx3WjOaxkU5HY5SSvk1txKAiNwlIutFZJ2IvC0iUSLyOxFZaU+bLiJh3g72s7V7OZpbpFf+\nKqWUB1SZAESkJfB/QIoxpjsQCkwApgPj7WnpwGRvBgowe2kG7RJiGdQ+3tubUkqpgOduE1AYEG2f\n5ccAOUCBMWaL/f4C4FIvxPeLXSdKSUvPYsKAJES081cppWqrymYbY8xuEfkbkAHkAV8B7wLPikiK\nMWYFMA5oXd7yIjIFmAKQmJhIampqjQL9akcuYSFCs7x0UlMzarQOpWoqOzu7xmVXKV9VZQIQkUbA\nKKAtcBR4D7gKGA/8Q0QisZJCcXnLG2OmAdMAUlJSzIgRI6odZE5BMTcv+JJLerVk5Lm9q728UrWV\nmppKTcquUr7MnY7bs4GfjTEHAUTkA2CwMWYWMNSedi7QyVtBzlu9h/wSuGqgdv4qpZSnuNMHkAEM\nFJEYsRrfzwI2ikhTALsGcB/wmreC3Hssnzb1Q+iT1Mhbm1BKqaDjTh/AUhF5H1iJ1czzE1aTzpMi\nMhIribxqjPnWW0HefU4neoXu1s5fpZTyILfG7htjHgEeKTP5j/ZfndDbPiillGf51ZXASimlPEcT\ngFJKBSlNAEopFaQ0ASilVJDSBKCUUkFKE4BSSgUpTQBKKRWkxBhTdxsTOQZsrWSWBsCxCt5LAA55\nPKi6U9ln85dt1mZ9NVm2Osu4M29V8wRy+YO6L4Na/qo3T2XvJxtjPP8QdGNMnf0B02r6PrCiLmOt\n68/uD9uszfpqsmx1lnFn3mAuf94oD3W9vWAuf976q+smoPm1fN+fOfHZPL3N2qyvJstWZxl35g3m\n8gd1//m0/FVvnjovf3XaBFQbIrLCGJPidBwqOGn5U4HInzqBpzkdgApqWv5UwPGbGoBSSinP8qca\ngFJKKQ/SBKCUUkFKE4BSSgWpgEkAIhIrImn2U8qUqjMi0lVEXhOR90XkFqfjUcpdjicAEfmPiBwQ\nkXVlpp8vIptFZJuI3O/Gqu4D3vVOlCpQeaL8GWM2GmNuBi4HdKio8huOjwISkWFANjDDGNPdnhYK\nbAHOATKB5cCVQCjwlzKruA7oiXWpfhRwyBjzSd1Er/ydJ8qfMeaAiFwC3A9MNca8VVfxK1Ubbj0T\n2JuMMd+JSJsyk/sD24wxOwBEZA4wyhjzF+A3TTwiciYQC3QD8kTkM2NMqVcDVwHBE+XPXs88YJ6I\nfApoAlB+wfEEUIGWwC6X15nAgIpmNsb8CUBErsGqAejBX9VGtcqfiIwAxgKRwGdejUwpD/LVBCDl\nTKuyrcoY86bnQ1FBqFrlzxiTCqR6KxilvMXxTuAKZAKtXV63AvY4FIsKPlr+VFDw1QSwHOgoIm1F\nJAIYD8xzOCYVPLT8qaDgeAIQkbeBxUBnEckUkeuNMcXA7cCXwEbgXWPMeifjVIFJy58KZo4PA1VK\nKeUMx2sASimlnKEJQCmlgpQmAKWUClKaAJRSKkhpAlBKqSClCUAppYKUJgCllApSmgCUUipIaQJQ\nSqkg9f8Degh0oKYhHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10be80350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum accuracy is 0.00% with L2 Regularization constant of 0.000000\n"
     ]
    }
   ],
   "source": [
    "print('Plot the L2 Regularization loss for our Test')\n",
    "plt.semilogx(l2_constant_values, accuracy_values)\n",
    "plt.grid(True)\n",
    "plt.title('Accuracy against L2 regularization (1 Layer Neural Network)')\n",
    "plt.show()\n",
    "print('Maximum accuracy is %.2f%% with L2 Regularization constant of %f' % (max_accuracy, best_l2_constant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using L2 Regularization for Neural Network Model (1 Layer) with overfitting')\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "# Buildig the Network\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_l2_feature = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    layer1_biases = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    layer2_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    layer2_biases = tf.Variable(tf.zeros([num_labels]))    \n",
    "\n",
    "    # Training computation.\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, layer1_weights) + layer1_biases)\n",
    "    logits = tf.matmul(hidden_layer, layer2_weights) + layer2_biases\n",
    "    loss = tf.reduce_mean( \\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    tf_l2_feature * (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    hidden_layer_valid_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, layer1_weights) + layer1_biases)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_layer_valid_prediction, layer2_weights) + layer2_biases)\n",
    "    hidden_layer_test_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, layer1_weights) + layer1_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_layer_test_prediction, layer2_weights) + layer2_biases)\n",
    "\n",
    "print('Tensorflow Graph created')\n",
    "\n",
    "num_steps = 101\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: best_l2_constant}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using L2 Regularization for Neural Network Model (1 Layer) with Dropout')\n",
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "# Buildig the Network\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_l2_feature = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    layer1_biases = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    layer2_weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    layer2_biases = tf.Variable(tf.zeros([num_labels]))    \n",
    "\n",
    "    # Training computation.\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, layer1_weights) + layer1_biases)\n",
    "    hidden_layer_with_dropout = tf.nn.dropout(hidden_layer)\n",
    "    logits = tf.matmul(hidden_layer_with_dropout, layer2_weights) + layer2_biases\n",
    "    loss = tf.reduce_mean( \\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    tf_l2_feature * (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    hidden_layer_valid_prediction = tf.nn.relu(tf.matmul(tf_valid_dataset, layer1_weights) + layer1_biases)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_layer_valid_prediction, layer2_weights) + layer2_biases)\n",
    "    hidden_layer_test_prediction = tf.nn.relu(tf.matmul(tf_test_dataset, layer1_weights) + layer1_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_layer_test_prediction, layer2_weights) + layer2_biases)\n",
    "\n",
    "print('Tensorflow Graph created')\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_l2_feature: 1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
